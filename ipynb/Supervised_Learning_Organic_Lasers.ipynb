{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def extractor(url,wait_time):\n",
    "    \"\"\"\n",
    "    Accepts a url and stores its html code before parsing and extracting the abstract and date as text.\n",
    "    Feeds directly into parser, so don't call this function unless you want to obtain the abstract\n",
    "    from a single url.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    #driver.find_element_by_id('').send_keys('')#\n",
    "    #driver.find_element_by_id ('').send_keys('')#\n",
    "    #driver.find_element_by_id('submit').click()#\n",
    "    time.sleep(wait_time) # important\n",
    "\n",
    "    html_doc = driver.page_source # stores the source HTML code in the driver's page_source attribute\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    abstract = soup.find('div', {'class':\"abstract author\"}).find('p').text\n",
    "    pub_date = soup.find('meta', {'name': \"citation_publication_date\"})['content']\n",
    "    \n",
    "    driver.quit()\n",
    "    return pub_date, abstract\n",
    "\n",
    "\n",
    "def parse_all(driver):\n",
    "    \"\"\"\n",
    "    The following method is designed to automatically parse each url contained in a long list \n",
    "    of scraped urls, and writes the title, abstract, and doi to a new text file with a user\n",
    "    input \"file_name.txt.\"\n",
    "    \"\"\"\n",
    "    url_lst = input(\"Enter name of file with .txt extension with list of urls: \")\n",
    "    data = pd.read_csv(url_lst,header=None,names=['url']) #text file containing a list of the scraped urls (should be in same directory)\n",
    "    file_name = input(\"Input the file name with .txt extension you wish to store abstracts in: \")\n",
    "    file = open(file_name,'w')\n",
    "\n",
    "    max_iters = len(data) #total number of scraped urls to be parsed\n",
    "    print(\"The parser will parse: \" + str(max_iters) + \" urls.\")\n",
    "\n",
    "    for i in range(0,max_iters):\n",
    "        print('On url ',i)\n",
    "        driver.refresh()\n",
    "        time.sleep(2)\n",
    "        urli = str(extractor(data.iloc[i,0],3))\n",
    "        file.write(urli)\n",
    "        file.write('\\n')\n",
    "    driver.quit()\n",
    "\n",
    "    return file_name\n",
    "\n",
    "### Actual executable code is shown below ###\n",
    "### driver = webdriver.Chrome()\n",
    "### parse_all(driver)\n",
    "\n",
    "def tokenizer(file_name):\n",
    "    \"\"\" \n",
    "    Accepts text file as a string (e.g. \"abstracts.txt\") containing a list of abstracts as input and cleans up text using regex.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        corpus = file.readlines()\n",
    "        processed_abstracts = [w.lower() for w in corpus]\n",
    "        processed_abstracts = [re.sub('[^a-zA-Z]', ' ', w) for w in processed_abstracts]\n",
    "        processed_abstracts = [re.sub(r'\\s+', ' ', w) for w in processed_abstracts]\n",
    "    tokens = [nltk.word_tokenize(sent) for sent in processed_abstracts]\n",
    "\n",
    "    for i in range(len(processed_abstracts)):\n",
    "        tokens[i] = [w for w in tokens[i] if w not in stopwords.words('english')]\n",
    "   \n",
    "    # Passes all tokens to Word2Vec to train model\n",
    "    model = Word2Vec(tokens, min_count=2)\n",
    "    print(model)\n",
    "    vocabulary = list(model.wv.vocab)\n",
    "\n",
    "    return model, vocabulary\n",
    "\n",
    "def single_abstract_tkzr(abstract):\n",
    "    \"\"\"\n",
    "    This method tokenizes an abstract from a single url. Wait time is an integer number of seconds you\n",
    "    want to wait for the page to load.\n",
    "    \"\"\"\n",
    "    #driver = webdriver.Chrome()\n",
    "    #abstract_text = extractor(url,wait_time)\n",
    "    test_abstract = abstract.lower()\n",
    "    test_abstract = re.sub('[^a-zA-Z]', ' ', test_abstract) \n",
    "    test_abstract = re.sub(r'\\s+', ' ', test_abstract)\n",
    "        \n",
    "    abstract_tokens = nltk.word_tokenize(test_abstract)\n",
    "\n",
    "    text_tokens = [tkn for tkn in abstract_tokens if tkn not in stopwords.words('english')]\n",
    "    #driver.quit()\n",
    "    return text_tokens\n",
    "\n",
    "def cosine_scores(search_terms,text_tokens,model,n_tokens):\n",
    "    \"\"\"\n",
    "    Extracts the top n most similar tokens and their respective cosine similarity scores by \n",
    "    comparing the tokens from a single abstract to the trained vocabulary.\n",
    "    Parameters:\n",
    "    search_terms: desired search terms written as a list of strings; \n",
    "    text_tokens: A list of tokens for the text_tokens;\n",
    "    model: The trained word2vec model;\n",
    "    n_tokens: the number of top most similar tokens you'd like to return\n",
    "    \"\"\"\n",
    "\n",
    "    store = defaultdict(int)\n",
    "    for word in search_terms:\n",
    "        for tkn in text_tokens:\n",
    "            store[tkn] += model.wv.similarity(word,tkn)\n",
    "    \n",
    "    # Orders dictionary from highest to lowest cosine similarity score\n",
    "    cos_scores = sorted(store.items() , reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # Extracts top 20 most similar tokens\n",
    "    return cos_scores[:int(n_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"organic.laser.url.csv\")\n",
    "\n",
    "first_condition = df['PDF only'].isnull() #NaN actually means that nothing is there, not a string\n",
    "second_condition = df['Score AC'] >= 0.0 \n",
    "third_condition = df['Score TG'] >= 0.0\n",
    "fourth_condition = df['Score WT'] >= 0.0\n",
    "keep = (first_condition & (second_condition | third_condition | fourth_condition )) #Masking of URLs with PDFs\n",
    "\n",
    "new_df = df[keep]\n",
    "new_df = new_df.reindex(columns = ['Date', 'URL', 'Abstract', 'Tokens', 'Cosine Top 20', 'Cumulative Sum', 'Score AC', 'Score TG', 'Score WT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "for i in range(0, len(new_df)):\n",
    "    print('On url', i)\n",
    "    driver.refresh()\n",
    "    time.sleep(2)\n",
    "    pub_date, abstract = extractor(new_df['URL'].iloc[i],3) #Extracting both publication date and abstract\n",
    "    new_df.iloc[i,2] = str(abstract) #Date is position 0 and Abstract is position 2 in the array\n",
    "    new_df.iloc[i,0] = str(pub_date)\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "#Need to save new_df as csv for later and abstracts as txt for tokenizer\n",
    "new_df.to_csv('Organic_Laser_Abstracts.csv')\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Abstracts.csv', index_col=[0])\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df.iloc[i,3] = str(single_abstract_tkzr(new_df.iloc[i,2])) #Single_abstract_tkzr repurposed for abstracts already in new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Organic_Laser_Tokens.csv', index=False) #Saving new_df after adding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = ''\n",
    "for i in range(0, len(new_df)):\n",
    "    abstracts += str(new_df.iloc[i,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Organic_Laser_Tokenizer.txt', \"w\") as output: #Save .txt file for tokenizer\n",
    "    output.write(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Tokens.csv')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = new_df\n",
    "for i in range(0,len(test_df)):\n",
    "    test_df.iloc[i,3] = new_df.iloc[i,3][1:-1] #Getting rid of the brackets around the tokens in Tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(new_df)):\n",
    "    new_df.iloc[i,3] = new_df.iloc[i,3][1:-1] #Getting rid of the brackets around the tokens in Tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Organic_Laser_Tokens2.csv', index=False) #Saving new_df after eliminating brackets in Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Tokens2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(word):\n",
    "    word = word[1:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Word2Vec model\n",
    "tokens = []\n",
    "for i in range(0,len(new_df)):\n",
    "    string_of_tokens = new_df.iloc[i,3]\n",
    "    string_of_tokens = string_of_tokens.split(', ')\n",
    "    new_tokens = list(map(remove_apostrophe, string_of_tokens)) #Removing the apostrophes in each token\n",
    "    tokens.append(new_tokens) #\"tokens\" is the list of tokens from each abstract to be used for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add words manually, not needed in the end\n",
    "tokens.append(['octyloxy']) \n",
    "tokens.append(['bianphthalene'])\n",
    "tokens.append(['tolyl'])\n",
    "tokens.append(['vinylbenzene'])\n",
    "tokens.append(['dpavb'])\n",
    "tokens.append(['pile'])\n",
    "tokens.append(['quaterthiophenes'])\n",
    "tokens.append(['iradiation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=13711, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(tokens, size=100, min_count=1, iter=30) #training model with token list from above\n",
    "vocabulary = list(model.wv.vocab) #saving vocabulary as list for visualization\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cosine score\n",
    "tokens_cs = []\n",
    "for i in range(0,len(new_df)):\n",
    "    string_of_tokens_cs = new_df.iloc[i,3]\n",
    "    string_of_tokens_cs = string_of_tokens_cs.split(', ')\n",
    "    new_tokens_cs = list(map(remove_apostrophe, string_of_tokens_cs)) #Removing the apostrophes in each token for cosine score\n",
    "    tokens_cs.extend(new_tokens_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatized tokens for cosine score, was not source of error, turned out to be min_count\n",
    "lemmatized_tokens = []\n",
    "lemmatized_tokens = list(map(lemmatizer.lemmatize, tokens_cs))\n",
    "lemmatized_tokens.remove('successively') #'successively' would not be lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling out Cosine Top 20 column\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df['Cosine Top 20'].loc[i] = cosine_scores(['organic','laser'],tokens[i],model,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling out Cumulative Sum column\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df['Cumulative Sum'].loc[i] = sum(i for j, i in new_df['Cosine Top 20'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = new_df\n",
    "data_df.to_csv('NLP_Organic_Laser_Data.csv', index=False) #Saving data_df after filling out all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('NLP_Organic_Laser_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Year vs score plot#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
