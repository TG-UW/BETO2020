{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "def extractor(url,wait_time):\n",
    "    \"\"\"\n",
    "    Accepts a url and stores its html code before parsing and extracting the abstract as text.\n",
    "    Feeds directly into parser, so don't call this function unless you want to obtain the abstract\n",
    "    from a single url.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    #driver.find_element_by_id('').send_keys('')#\n",
    "    #driver.find_element_by_id ('').send_keys('')#\n",
    "    #driver.find_element_by_id('submit').click()#\n",
    "    time.sleep(wait_time) # important\n",
    "\n",
    "    html_doc = driver.page_source # stores the source HTML code in the driver's page_source attribute\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    abstract = soup.find('div', {'class':\"abstract author\"}).find('p').text\n",
    "    \n",
    "    driver.quit()\n",
    "    return abstract\n",
    "\n",
    "def parse_all(driver):\n",
    "    \"\"\"\n",
    "    The following method is designed to automatically parse each url contained in a long list \n",
    "    of scraped urls, and writes the title, abstract, and doi to a new text file with a user\n",
    "    input \"file_name.txt.\"\n",
    "    \"\"\"\n",
    "    url_lst = input(\"Enter name of file with .txt extension with list of urls: \")\n",
    "    data = pd.read_csv(url_lst,header=None,names=['url']) #text file containing a list of the scraped urls (should be in same directory)\n",
    "    file_name = input(\"Input the file name with .txt extension you wish to store abstracts in: \")\n",
    "    file = open(file_name,'w')\n",
    "\n",
    "    max_iters = len(data) #total number of scraped urls to be parsed\n",
    "    print(\"The parser will parse: \" + str(max_iters) + \" urls.\")\n",
    "\n",
    "    for i in range(0,max_iters):\n",
    "        print('On url ',i)\n",
    "        driver.refresh()\n",
    "        time.sleep(2)\n",
    "        urli = str(extractor(data.iloc[i,0],3))\n",
    "        file.write(urli)\n",
    "        file.write('\\n')\n",
    "    driver.quit()\n",
    "\n",
    "    return file_name\n",
    "\n",
    "### Actual executable code is shown below ###\n",
    "### driver = webdriver.Chrome()\n",
    "### parse_all(driver)\n",
    "\n",
    "def tokenizer(file_name):\n",
    "    \"\"\" \n",
    "    Accepts text file as a string (e.g. \"abstracts.txt\") containing a list of abstracts as input and cleans up text using regex.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        corpus = file.readlines()\n",
    "        processed_abstracts = [w.lower() for w in corpus]\n",
    "        processed_abstracts = [re.sub('[^a-zA-Z]', ' ', w) for w in processed_abstracts]\n",
    "        processed_abstracts = [re.sub(r'\\s+', ' ', w) for w in processed_abstracts]\n",
    "    tokens = [nltk.word_tokenize(sent) for sent in processed_abstracts]\n",
    "\n",
    "    for i in range(len(processed_abstracts)):\n",
    "        tokens[i] = [w for w in tokens[i] if w not in stopwords.words('english')]\n",
    "\n",
    "    # Passes all tokens to Word2Vec to train model\n",
    "    model = Word2Vec(tokens, size=100, min_count=2, iter=10) \n",
    "    vocabulary = model.wv.vocab\n",
    "\n",
    "    return model, vocabulary\n",
    "\n",
    "def single_abstract_tkzr(url,wait_time):\n",
    "    \"\"\"\n",
    "    This method tokenizes an abstract from a single url. Wait time is an integer number of seconds you\n",
    "    want to wait for the page to load.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    abstract_text = extractor(url,wait_time)\n",
    "    test_abstract = abstract_text.lower()\n",
    "    test_abstract = re.sub('[^a-zA-Z]', ' ', test_abstract) \n",
    "    test_abstract = re.sub(r'\\s+', ' ', test_abstract)\n",
    "        \n",
    "    abstract_tokens = nltk.word_tokenize(test_abstract)\n",
    "\n",
    "    text_tokens = [tkn for tkn in abstract_tokens if tkn not in stopwords.words('english')]\n",
    "    driver.quit()\n",
    "    return text_tokens\n",
    "\n",
    "def cosine_scores(search_terms,text_tokens,model,n_tokens):\n",
    "    \"\"\"\n",
    "    Extracts the top n most similar tokens and their respective cosine similarity scores by \n",
    "    comparing the tokens from a single abstract to the trained vocabulary.\n",
    "    Parameters:\n",
    "    search_terms: desired search terms written as a list of strings; \n",
    "    text_tokens: A list of tokens for the text_tokens;\n",
    "    model: The trained word2vec model;\n",
    "    n_tokens: the number of top most similar tokens you'd like to return\n",
    "    \"\"\"\n",
    "\n",
    "    store = defaultdict(int)\n",
    "    for word in search_terms:\n",
    "        for tkn in text_tokens:\n",
    "            store[tkn] += model.wv.similarity(word,tkn)\n",
    "    \n",
    "    # Orders dictionary from highest to lowest cosine similarity score\n",
    "    cos_scores = sorted(store.items() , reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # Extracts top 20 most similar tokens\n",
    "    return cos_scores[:int(n_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"organic.laser.url.csv\")\n",
    "\n",
    "#df.loc[df['PDF only'] == 0, 'PDF only'] = 'Nan'; for changing the values of 'PDF only' from 0 to 'NaN'#\n",
    "\n",
    "first_condition = df['PDF only'] == 0.0\n",
    "second_condition = df['Score AC'] >= 0.0 \n",
    "third_condition = df['Score TG'] >= 0.0\n",
    "fourth_condition = df['Score WT'] >= 0.0\n",
    "msk = (first_condition & (second_condition | third_condition | fourth_condition ))\n",
    "\n",
    "new_df = df[msk]\n",
    "new_df = new_df.reindex(columns = ['URL', 'Abstract', 'Tokens', 'Cosine Top 20', 'Cumulative Sum', 'Score AC', 'Score TG', 'Score WT'])\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print('On url', i)\n",
    "    driver.refresh()\n",
    "    time.sleep(2)\n",
    "    new_df['Abstract'].iloc[i] = str(extractor(new_df['URL'].iloc[i],3))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#Need to save new_df and abstracts.txt\n",
    "\n",
    "print (new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://www-sciencedirect-com.offcampus.lib.washington.edu/science/article/pii/S1566119911002801'\n",
    "\n",
    "print(str(extractor(test_url, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Organic_Laser_Abstracts.csv')\n",
    "new_df['Abstracts'].to_txt('Organic_Laser_Tokenizer.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have tokens added to the new_df and teach Word2Vec model#\n",
    "\n",
    "#new_df = pd.read_csv('.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
