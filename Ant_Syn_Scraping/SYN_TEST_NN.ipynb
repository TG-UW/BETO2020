{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec as wv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "#import PhysicallyInformedLossFunction as PhysLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening contents of Word2Vec model1\n",
    "data = '/Users/Thomas/Desktop/BETO2020-master/Ant_Syn_Scraping/all_abstracts_model'\n",
    "os.chdir(data)\n",
    "model1 = wv.load('all_abstract_model.model')\n",
    "vocabulary1 = list(model1.wv.vocab)\n",
    "#use model.build_vocab(sentence, update=True) to add missing words to model's vocabulary?\n",
    "#or delete the rows that yield the KeyError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening contents of Word2Vec model2\n",
    "data = '/Users/Thomas/Desktop/BETO2020-master/Ant_Syn_Scraping/3j_15e_50D_min1_model'\n",
    "os.chdir(data)\n",
    "model2 = wv.load('3j_15e_50D_min1_model.model')\n",
    "vocabulary2 = list(model2.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/Thomas/Desktop/BETO2020-master/Ant_Syn_Scraping/'\n",
    "os.chdir(data)\n",
    "carbon_df = pd.read_excel('Carbon_SynAntList_Full_Refined_copy.xlsx', skip_rows=1, nrows=2000, index_col=0)\n",
    "carbon_df = carbon_df.rename(columns = {'Carbon_SynAntList_Full_Refined':'index', 'Unnamed: 1':'word 1', 'Unnamed: 2':'word 2','Unnamed: 3':'relationship', 'Unnamed: 4': 'label'})\n",
    "carbon_df = carbon_df.fillna(0)\n",
    "carbon_df = carbon_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(carbon_df['word 1'])\n",
    "list2 = list(carbon_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary1))) + list((set(list2).difference(vocabulary1)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "carbon_df = carbon_df[~carbon_df['word 1'].isin(missing)]\n",
    "carbon_df = carbon_df[~carbon_df['word 2'].isin(missing)]\n",
    "\n",
    "#reseting indeces after mask\n",
    "carbon_df.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(carbon_df)):\n",
    "    carbon_df['word 1'].iloc[i] = model1.wv.__getitem__(str(carbon_df['word 1'].iloc[i]))\n",
    "    carbon_df['word 2'].iloc[i] = model1.wv.__getitem__(str(carbon_df['word 2'].iloc[i]))\n",
    "    \n",
    "    if carbon_df['relationship'].iloc[i] == 'syn' and carbon_df['label'].iloc[i] == 1:\n",
    "        carbon_df['relationship'].iloc[i] = 1\n",
    "    else:\n",
    "        carbon_df['relationship'].iloc[i] = 0      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df.to_json('SYN_NN_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df = pd.read_json('SYN_NN_DATA.json', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzipping the vectors for 'word 1' and 'word 2' into different dataframes\n",
    "\n",
    "x1_train_array = np.empty((len(x_train),50))\n",
    "x1_train_df = pd.DataFrame(data=x1_train_array[0:,:])\n",
    "\n",
    "x1_test_array = np.empty((len(x_test),50))\n",
    "x1_test_df = pd.DataFrame(data=x1_test_array[0:,:])\n",
    "\n",
    "x2_train_array = np.empty((len(x_train),50))\n",
    "x2_train_df = pd.DataFrame(data=x2_train_array[0:,:])\n",
    "\n",
    "x2_test_array = np.empty((len(x_test),50))\n",
    "x2_test_df = pd.DataFrame(data=x2_test_array[0:,:])\n",
    "\n",
    "for i in range(len(x1_train_df)):\n",
    "    for u in range(len(x_train['word 1'][i])):\n",
    "        x1_train_df.iloc[i,u] = x_train['word 1'][i][u]\n",
    "\n",
    "for i in range(len(x1_test_df)):\n",
    "    for u in range(len(x_test['word 1'][i])):\n",
    "        x1_test_df.iloc[i,u] = x_test['word 1'][i][u]\n",
    "        \n",
    "for i in range(len(x2_train_df)):\n",
    "    for u in range(len(x_train['word 2'][i])):\n",
    "        x2_train_df.iloc[i,u] = x_train['word 2'][i][u]\n",
    "        \n",
    "for i in range(len(x2_test_df)):\n",
    "    for u in range(len(x_test['word 2'][i])):\n",
    "        x2_test_df.iloc[i,u] = x_test['word 2'][i][u]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzipping the vectors for the whole x_train and x_test datasets\n",
    "#x_train_array = np.empty((len(x_train),100))\n",
    "#x_train_df = pd.DataFrame(data=x_train_array[0:,:])\n",
    "\n",
    "#x_test_array = np.empty((len(x_test),100))\n",
    "#x_test_df = pd.DataFrame(data=x_test_array[0:,:])\n",
    "\n",
    "#for i in range(len(x_train_df)):\n",
    " #   for u in range(len(x_train['word 2'][i])):\n",
    "  #      x_train_df.iloc[i,u] = x_train['word 1'][i][u]\n",
    "        \n",
    "#for i in range(len(x_train_df)):\n",
    " #   for u in range(len(x_train['word 2'][i])):\n",
    "  #      x_train_df.iloc[i,(50+u)] = x_train['word 2'][i][u]\n",
    "              \n",
    "#for i in range(len(x_test_df)):\n",
    " #   for u in range(len(x_test['word 1'][i])):\n",
    "  #      x_test_df.iloc[i,u] = x_test['word 1'][i][u]\n",
    "                       \n",
    "#for i in range(len(x_test_df)):\n",
    " #   for u in range(len(x_test['word 2'][i])):\n",
    "  #      x_test_df.iloc[i,(50+u)] = x_test['word 2'][i][u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1_train_tensor = torch.tensor(x1_train_df.values.astype(np.float32))#convert pd.DataFrame -> np.ndarray -> torch.tensor\n",
    "#x2_train_tensor = torch.tensor(x2_train_df.values.astype(np.float32))\n",
    "#syn_train_tensor = torch.tensor(syn_train.values.astype(np.float32))\n",
    "\n",
    "    \n",
    "#create tensor with features and targets\n",
    "#train_tensor = torch.utils.data.TensorDataset(torch.cat((x1_train_tensor, x2_train_tensor), dim=0), syn_train_tensor)\n",
    "#create iterable dataset with batches\n",
    "#training_data_set = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "#x1_test_tensor = torch.tensor(x1_test_df.values.astype(np.float32))\n",
    "#x2_test_tensor = torch.tensor(x2_test_df.values.astype(np.float32))\n",
    "#syn_test_tensor = torch.tensor(syn_test.values.astype(np.float32))\n",
    "#nonsyn_test_tensor = torch.tensor(nonsyn_test)\n",
    "\n",
    "#test_tensor = torch.utils.data.TensorDataset(x1_test_tensor, x2_test_tensor, syn_test_tensor)\n",
    "#testing_data_set = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = carbon_df[['word 1', 'word 2']] #Input features used to make predictions\n",
    "Y = carbon_df[['relationship']] #Target feature to be predicted \n",
    " \n",
    "x_train, x_test, syn_train, syn_test = train_test_split(X,Y, test_size = 0.2, shuffle = True) #split dataset into separate testing and training datasets\n",
    "\n",
    "x_train.reset_index(inplace = True)\n",
    "x_test.reset_index(inplace = True)\n",
    "syn_train.reset_index(inplace = True)\n",
    "syn_test.reset_index(inplace = True)\n",
    "\n",
    "x1_train = x_train['word 1']\n",
    "x2_train = x_train['word 2']\n",
    "x1_test = x_test['word 1']\n",
    "x2_test = x_test['word 2']\n",
    "\n",
    "#Will probably need to save and load the x_train, x_test, syn_train, syn_test\n",
    "#and call that in the respective train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carbon_SynAntList_Full_Refined</th>\n",
       "      <th>word 1</th>\n",
       "      <th>word 2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0246487856, -5.6508703232, -1.4263287783, -...</td>\n",
       "      <td>[0.3912109137, -2.6639938354, -0.4191870987000...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[1.0246487856, -5.6508703232, -1.4263287783, -...</td>\n",
       "      <td>[0.6780726314000001, -0.077852197, 3.356479167...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[1.0246487856, -5.6508703232, -1.4263287783, -...</td>\n",
       "      <td>[-0.40175724030000004, 0.6633739471, -1.507220...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[1.0246487856, -5.6508703232, -1.4263287783, -...</td>\n",
       "      <td>[2.6374275684, -0.8799803257000001, 1.95807564...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[1.0246487856, -5.6508703232, -1.4263287783, -...</td>\n",
       "      <td>[-1.5558528900000002, 2.8244459629, -3.4161539...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>[-0.2082266062, 1.197119236, 1.1562068462, -5....</td>\n",
       "      <td>[-1.9820196629, 0.9932372570000001, 1.02549302...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>[-0.2082266062, 1.197119236, 1.1562068462, -5....</td>\n",
       "      <td>[0.3834728599, 0.7061271071, -0.00781521570000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>[-0.2082266062, 1.197119236, 1.1562068462, -5....</td>\n",
       "      <td>[-2.8429896832, 1.9741824865000002, -0.3001447...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>[-0.2082266062, 1.197119236, 1.1562068462, -5....</td>\n",
       "      <td>[0.26439201830000003, 0.44663661720000003, -0....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>[-0.2082266062, 1.197119236, 1.1562068462, -5....</td>\n",
       "      <td>[-7.4225053787, -2.030264616, 0.6465284228, -7...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Carbon_SynAntList_Full_Refined  \\\n",
       "0                                1.0   \n",
       "1                                2.0   \n",
       "2                                3.0   \n",
       "3                                4.0   \n",
       "4                                7.0   \n",
       "...                              ...   \n",
       "1595                          1994.0   \n",
       "1596                          1995.0   \n",
       "1597                          1996.0   \n",
       "1598                          1997.0   \n",
       "1599                          1998.0   \n",
       "\n",
       "                                                 word 1  \\\n",
       "0     [1.0246487856, -5.6508703232, -1.4263287783, -...   \n",
       "1     [1.0246487856, -5.6508703232, -1.4263287783, -...   \n",
       "2     [1.0246487856, -5.6508703232, -1.4263287783, -...   \n",
       "3     [1.0246487856, -5.6508703232, -1.4263287783, -...   \n",
       "4     [1.0246487856, -5.6508703232, -1.4263287783, -...   \n",
       "...                                                 ...   \n",
       "1595  [-0.2082266062, 1.197119236, 1.1562068462, -5....   \n",
       "1596  [-0.2082266062, 1.197119236, 1.1562068462, -5....   \n",
       "1597  [-0.2082266062, 1.197119236, 1.1562068462, -5....   \n",
       "1598  [-0.2082266062, 1.197119236, 1.1562068462, -5....   \n",
       "1599  [-0.2082266062, 1.197119236, 1.1562068462, -5....   \n",
       "\n",
       "                                                 word 2  relationship label  \n",
       "0     [0.3912109137, -2.6639938354, -0.4191870987000...           1.0     1  \n",
       "1     [0.6780726314000001, -0.077852197, 3.356479167...           1.0     1  \n",
       "2     [-0.40175724030000004, 0.6633739471, -1.507220...           0.0     0  \n",
       "3     [2.6374275684, -0.8799803257000001, 1.95807564...           0.0     0  \n",
       "4     [-1.5558528900000002, 2.8244459629, -3.4161539...           0.0     0  \n",
       "...                                                 ...           ...   ...  \n",
       "1595  [-1.9820196629, 0.9932372570000001, 1.02549302...           0.0     0  \n",
       "1596  [0.3834728599, 0.7061271071, -0.00781521570000...           0.0     0  \n",
       "1597  [-2.8429896832, 1.9741824865000002, -0.3001447...           0.0     0  \n",
       "1598  [0.26439201830000003, 0.44663661720000003, -0....           0.0     0  \n",
       "1599  [-7.4225053787, -2.030264616, 0.6465284228, -7...           0.0     0  \n",
       "\n",
       "[1600 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carbon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is to inspect what is being done in the SYN_Train_Dataset cell\n",
    "\n",
    "#train_samples = [[] for i in range(len(x_train))]\n",
    "        \n",
    "#x1_train_array = np.empty((len(x_train),50))\n",
    "#x1_train_df = pd.DataFrame(data=x1_train_array[0:,:])\n",
    "#x2_train_array = np.empty((len(x_train),50))\n",
    "#x2_train_df = pd.DataFrame(data=x2_train_array[0:,:])\n",
    "       \n",
    "#for i in range(len(x1_train_df)):\n",
    " #   for u in range(len(x_train['word 1'][i])):\n",
    "  #      x1_train_df.iloc[i,u] = x_train['word 1'][i][u]\n",
    "   #     x2_train_df.iloc[i,u] = x_train['word 2'][i][u]\n",
    "\n",
    "#for i in range(len(x1_train_df)):\n",
    " #   train_samples[i] = [x1_train_df.iloc[i,:].tolist(), x2_train_df.iloc[i,:].tolist()], syn_train['relationship'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYN_Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #we should probably load x_train, syn_train here after saving it after datasplit\n",
    "        self.train_samples = [[] for i in range(len(x_train))]\n",
    "        \n",
    "        x1_train_array = np.empty((len(x_train),50))\n",
    "        x1_train_df = pd.DataFrame(data=x1_train_array[0:,:])\n",
    "        x2_train_array = np.empty((len(x_train),50))\n",
    "        x2_train_df = pd.DataFrame(data=x2_train_array[0:,:])\n",
    "       \n",
    "        for i in range(len(x1_train_df)):\n",
    "            for u in range(len(x_train['word 1'][i])):\n",
    "                x1_train_df.iloc[i,u] = x_train['word 1'][i][u]\n",
    "                x2_train_df.iloc[i,u] = x_train['word 2'][i][u] #save what x1 and x2 train_df look like outside the dataset\n",
    "\n",
    "        for i in range(len(x1_train_df)):\n",
    "            self.train_samples[i] = [x1_train_df.iloc[i,:].tolist(), x2_train_df.iloc[i,:].tolist()], syn_train['relationship'][i]\n",
    "            #self.train_samples[i].append(x2_train_df.iloc[i,:].tolist())\n",
    "            #self.train_samples[i].append(syn_train['relationship'][i])\n",
    "        \n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.train_samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.train_samples[index]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = SYN_Train_Dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0.9973859787, 2.3988208771, -3.0376405716, 0.8158480525, -1.6229007244, 2.8811962605, -0.9127098918000001, -2.7043669224, 0.12181437760000001, -0.5133670568, -1.649153471, -2.9331746101, 4.188202858, 3.0615022182, -2.3529634476, 0.43747541310000004, 3.2393746376, -1.5008270741, 1.1484955549, -1.5229882002, 3.7909066677, -0.7344305515, 2.5107870102, 2.8808379173, 1.1797090769, 0.4120798409, 2.9954190254, -2.3725163937, 3.0012643337, 2.3138353825, 0.8126859069, -0.7852016091, 3.0251886845, 1.9373266697, 0.4772277176, 0.41635900740000004, -7.3228983879000005, 2.3539719582, -0.0728377998, -1.4630441666, -1.9475537539, 0.4102407992, 3.4997196198, 0.0801593214, 2.729136467, -1.2905237675, 3.8139734268, -3.6601963043, 3.981307745, 5.4369807243], [0.7669314742000001, 0.2842409611, -1.5693807602, -2.947496891, -0.5005902052, -0.9371783137, -0.7468199134, 3.4563672543, 2.7787907124, -1.6619492769000002, -0.1412225217, -1.5308130979999999, -2.0223829746, -0.7647606730000001, 2.1933870316, -2.2380340099, 1.2460300922, 2.1491434574, -0.2214870602, -1.6341514587, -3.0880713463, 0.8313919306, -0.4976740181, 4.0175914764, 1.5011308193000001, -0.3238175511, 2.4479720592, 0.2140075415, 4.2057161331, 1.1366195679, -0.6085426807000001, -3.0977683067, 2.1628119946, -3.1089715958, 0.2943252027, -1.9003659487000002, 1.4134404659, -1.5362757444000001, 2.4092493057, 0.2244298905, -1.5126070976000001, -2.5294523239, -0.506821394, -2.0634346008, -2.2591664791, 1.0404869318, 4.4117183685, -2.2846550941, 0.6525883079, 2.5905208588]], 0.0)\n"
     ]
    }
   ],
   "source": [
    " print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYN_Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #we should probably load x_test, syn_test here after saving it after datasplit\n",
    "        self.test_samples = [[] for i in range(len(x_test))]\n",
    "        \n",
    "        x1_test_array = np.empty((len(x_test),50))\n",
    "        x1_test_df = pd.DataFrame(data=x1_test_array[0:,:])\n",
    "        x2_test_array = np.empty((len(x_test),50))\n",
    "        x2_test_df = pd.DataFrame(data=x2_test_array[0:,:])\n",
    "\n",
    "        for i in range(len(x1_test_df)):\n",
    "            for u in range(len(x_test['word 1'][i])):\n",
    "                x1_test_df.iloc[i,u] = x_test['word 1'][i][u]\n",
    "                x2_test_df.iloc[i,u] = x_test['word 2'][i][u]\n",
    "\n",
    "        for i in range(len(x1_test_df)):\n",
    "            self.test_samples[i] = [x1_test_df.iloc[i,:].tolist(), x2_test_df.iloc[i,:].tolist()], syn_test['relationship'][i]\n",
    "            \n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.test_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.test_samples[idx]\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   dataset = SYN_Test_Dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensor with features and targets\n",
    "#create iterable dataset with batches\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = SYN_Train_Dataset(), batch_size = batch_size, shuffle = True)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = SYN_Test_Dataset(), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our NN will start with two branches, one that will take in x1_train, and the other to take in x2_train. \n",
    "#After embedding the input vectors for a few layers in their respective branches, the branches converge to a single branch, \n",
    "#which is when you concatenate x1_out and x2_out into syn_out. after going through the rest of the network, \n",
    "#loss will be calculated by comparing syn_out with syn_train (or syn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train_tensor = torch.tensor(x1_train_df.values.astype(np.float32))\n",
    "x2_train_tensor = torch.tensor(x2_train_df.values.astype(np.float32))\n",
    "syn_train_tensor = torch.tensor(syn_train.values.astype(np.float32))\n",
    "x1_test_tensor = torch.tensor(x1_test_df.values.astype(np.float32))\n",
    "x2_test_tensor = torch.tensor(x2_test_df.values.astype(np.float32))\n",
    "syn_test_tensor = torch.tensor(syn_test.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYN_TEST(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super(SYN_TEST, self).__init__()    \n",
    "        \n",
    "        self.x1_branch = nn.Sequential(\n",
    "        nn.Linear(in_dims, 50)\n",
    "        nn.Dropout(0.2), #to limit overfitting\n",
    "        nn.Linear(50,100), #expand\n",
    "        nn.Linear(100,300),\n",
    "        nn.Linear(300,100),\n",
    "        nn.Linear(100,50)) #compress\n",
    "        \n",
    "        #nn.Softplus()\n",
    "        \n",
    "        self.x2_branch = nn.Sequential(\n",
    "        nn.Linear(in_dims, 50)\n",
    "        nn.Dropout(0.2), #to limit overfitting\n",
    "        nn.Linear(50,100), #expand\n",
    "        nn.Linear(100,300),\n",
    "        nn.Linear(300,100),\n",
    "        nn.Linear(100,50)) #compress\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layer = nn.Linear(100, 32)\n",
    "        self.hidden_layer1 = nn.Linear(32, 16)\n",
    "        \n",
    "        #output layer\n",
    "        self.output_layer = nn.Linear(16,2)\n",
    "   \n",
    "    def forward(self, x1_in, x2_in):\n",
    "       \n",
    "        #pass each embedded data through each branch\n",
    "        x1_out = self.x1_branch(x1_in)\n",
    "        x2_out = self.x2_branch(x2_in)\n",
    "        \n",
    "        #concatenate outputs\n",
    "        combined_out = torch.cat((x1_out, x2_out), dim = 1) #dim = 0 is end to end cat and dim = 1 is side by side cat\n",
    "        \n",
    "        #pass through hidden layers\n",
    "        out = self.hidden_layer(combined_out)\n",
    "        out = self.hidden_layer1(out)\n",
    "        \n",
    "        #subspace layer would be here\n",
    "        \n",
    "        #pass to output layer\n",
    "        syn_out = self.output_layer(out)\n",
    "        \n",
    "        return syn_out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    \n",
    "    syn_losses = []\n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    syn_criterion = nn.MSELoss()\n",
    "    \n",
    "    #for features, labels in training_data_set: \n",
    "    \n",
    "    #may need to change above \"features\" portion here to accomodate for our custom dataset\n",
    "    #below is proposed alternative\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        syn_out = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(syn_out, labels)\n",
    "        total_loss = syn_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss is: {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        #syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            syn_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(syn_out, labels)\n",
    "            \n",
    "            test_loss = syn_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "            #syn_acc = accuracy(syn_out)\n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        \n",
    "        #syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, #syn_epoch_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = SYN_TEST(in_dims = 50, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-57a0ff1259c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_train_epoch_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-2adc62f5977f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, training_data_set, optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#have been encountering an issue where the data is not a double() but a float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#zero out any gradients from prior loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #syn_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "epochs = np.arange(1, (num_epochs+1), 1)\n",
    "\n",
    "plt.plot(epochs, train_epoch_losses, c = 'k', label = 'training error')\n",
    "plt.plot(epochs, test_epoch_losses, c = 'r', label = 'testing error')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title(\"Total Training & Testing Error\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (8,6))\n",
    "# plt.plot(epochs, train_epoch_accuracy, c = 'k', label = 'training accuracy')\n",
    "#plt.plot(epochs, pce_test_epoch_accuracies, c = 'k', label = 'pce accuracy')\n",
    "#plt.plot(epochs, voc_test_epoch_accuracies, c = 'r', label = 'voc accuracy')\n",
    "#plt.plot(epochs, jsc_test_epoch_accuracies, c = 'g', label = 'jsc accuracy')\n",
    "#plt.plot(epochs, ff_test_epoch_accuracies, c = 'b', label = 'ff accuracy')\n",
    "#plt.legend(loc = 'lower right')\n",
    "#plt.title(\"Branch Mean Absolute Percent Error\")\n",
    "#ax.set_xlabel('Epoch')\n",
    "#ax.set_ylabel('MAPE')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlin = ylin = np.arange(0, 100, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "plt.scatter(PCE_out, pce_labels)\n",
    "plt.plot(xlin, ylin, c = 'k')\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.set_xlabel(\"Predictions\")\n",
    "ax.set_ylabel(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "plt.scatter(Voc_out, voc_labels)\n",
    "plt.plot(xlin, ylin, c = 'k')\n",
    "ax.set_xlim(0, 0.9)\n",
    "ax.set_ylim(0, 0.9)\n",
    "ax.set_xlabel(\"Predictions\")\n",
    "ax.set_ylabel(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "plt.scatter(Jsc_out, jsc_labels)\n",
    "plt.plot(xlin, ylin, c = 'k')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_xlabel(\"Predictions\")\n",
    "ax.set_ylabel(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "plt.scatter(FF_out, ff_labels)\n",
    "plt.plot(xlin, ylin, c = 'k')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlabel(\"Predictions\")\n",
    "ax.set_ylabel(\"Ground Truth\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
