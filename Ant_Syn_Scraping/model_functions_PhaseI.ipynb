{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Synonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the synonym subspace. A lower and higher bound for synonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"  \n",
    "    def __init__ (self):\n",
    "        super(Loss_Synonymy, self).__init__()\n",
    "        \n",
    "    def forward(self, S1_out, S2_out, synonymy_score):\n",
    "        \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "            \n",
    "        for x, a, b in zip(synonymy_score, S1_out, S2_out): #x=synonymy_score, a=S1_out, b=S2_out\n",
    "    \n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2))))) #assumed Euclidean Distance\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2,error)\n",
    "                \n",
    "        \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Antonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the antonym subspace. A lower and higher bound for antonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Loss_Antonymy, self).__init__()\n",
    "       \n",
    "    def forward(self, S2_out, A1_out, antonymy_score): \n",
    "        \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "    \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "        \n",
    "        for x, a, b in zip(antonymy_score, A1_out, S2_out): #x=antonymy_score, a=A1_out, b=S2_out (to ensure trans-transitivity)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2)))))\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2, error)\n",
    "                 \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        error_total.retain_grad()\n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Labels(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is the last portion of the general loss function. Here the predicted synonymy and antonymy scores\n",
    "    are concatenated and compared to the concatenated labeled synonymy and antonymy scores\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss_Labels, self).__init__()\n",
    "       \n",
    "    def forward(self, S1_out, synonymy_score, antonymy_score):\n",
    "        \n",
    "        result_list = torch.zeros(S1_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        for x, y in zip(synonymy_score, antonymy_score):\n",
    "            \n",
    "            #print(synonymy_score)\n",
    "            #print(antonymy_score)\n",
    "            error = torch.nn.functional.log_softmax(torch.stack((x,y),dim=0), dim=0)\n",
    "            error = torch.argmax(error).float()\n",
    "            #error.squeeze()\n",
    "            \n",
    "            error.requires_grad = True\n",
    "            error.retain_grad()\n",
    "            result_list[element_count] = error\n",
    "            element_count += 1\n",
    "            result = torch.neg(result_list.mean())\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2404e351f082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#feeding the model pretrained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEmbedding_Pre_Trained_Weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPhase_I_NN\u001b[0m \u001b[0mneural\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0mweights\u001b[0m \u001b[0musing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#feeding the model pretrained weights\n",
    "\n",
    "class Embedding_Pre_Trained_Weights(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the pre-training of the Phase_I_NN neural network weights using a list of words from which\n",
    "    a list of weights can be obtained. It is then converted that can then be embedded using the from_pretrained() \n",
    "    function into the NN model\n",
    "    \"\"\"\n",
    "    def __init__(self, words, model):\n",
    "        super(Embedding_Pre_Trained_Weights, self).__init__()\n",
    "    \n",
    "        for i in range(len(words)):\n",
    "            words[i] = model.wv.__getitem__(words[i]).tolist()\n",
    "    \n",
    "        weight = torch.tensor(words)\n",
    "    \n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "    \n",
    "    def forward(self, index):\n",
    "        \n",
    "        index_vector = self.embedding(torch.FloatTensor(index))\n",
    "        \n",
    "        return index_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the best fit for our learning rate and epochs\n",
    "def fit(model, lr, epochs):\n",
    "\n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    #empty list to hold loss per epoch\n",
    "    train_epoch_losses = []\n",
    "    syn_train_epoch_losses = []\n",
    "    ant_train_epoch_losses = []\n",
    "    label_train_epoch_losses = []\n",
    "\n",
    "    test_epoch_losses = []\n",
    "    syn_test_epoch_losses = []\n",
    "    ant_test_epoch_losses = []\n",
    "    label_test_epoch_losses = []\n",
    "\n",
    "\n",
    "    syn_test_epoch_accuracies = []\n",
    "    ant_test_epoch_accuracies = []\n",
    "    test_epoch_accuracies = []\n",
    "\n",
    "    #pce_test_epoch_r2 = []\n",
    "    #voc_test_epoch_r2 = []\n",
    "    #jsc_test_epoch_r2 = []\n",
    "    #ff_test_epoch_r2 = []\n",
    "    #test_epoch_r2s = []\n",
    "\n",
    "    save_epochs = np.arange(0, num_epochs, 5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('On epoch ', epoch)\n",
    "    \n",
    "        #save_dir = \"/Users/Thomas/Desktop/BETO2020-Remote/Ant_Syn_Scraping/\"\n",
    "        #model_name = \"Phase_I_II_NN\"\n",
    "        #model_path = save_dir+model_name+'*.pt'\n",
    "        #if epoch < 10:\n",
    "            #save_path = save_dir + model_name + '_epoch0' + str(epoch) + '.pt'\n",
    "        #else:\n",
    "            #save_path = save_dir + model_name + '_epoch' + str(epoch) + '.pt'\n",
    "        \n",
    "#     if glob.glob(model_path) != []:\n",
    "#         model_states = glob.glob(model_path)\n",
    "#         model_states = sorted(model_states)\n",
    "#         previous_model = model_states[-1]\n",
    "        \n",
    "#         model, optimizer = nuts.load_trained_model(previous_model, model, optimizer)\n",
    "    \n",
    "        train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "        train_epoch_losses.append(train_epoch_loss)\n",
    "        syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "        ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "        label_train_epoch_losses.append(label_train_epoch_loss)\n",
    "\n",
    "\n",
    "        test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "\n",
    "        test_epoch_losses.append(test_epoch_loss)\n",
    "        syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "        ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "        label_test_epoch_losses.append(label_test_epoch_loss)\n",
    "\n",
    "        #tot_tst_loss = sum(test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss)\n",
    "        #test_epoch_losses.append(tot_tst_loss)\n",
    "\n",
    "        syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "        ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "\n",
    "        tot_test_acc = (syn_epoch_acc + ant_epoch_acc)\n",
    "        test_epoch_accuracies.append(tot_test_acc)\n",
    "\n",
    "        print('Finished epoch ', epoch)\n",
    "\n",
    "    best_loss_indx = test_epoch_losses.index(min(test_epoch_losses))\n",
    "    best_acc_indx = test_epoch_accuracies.index(min(test_epoch_accuracies))\n",
    "\n",
    "    fit_results = {\n",
    "        'lr': lr,\n",
    "        'best_loss_epoch': best_loss_indx,\n",
    "        'best_acc_epoch': best_acc_indx,\n",
    "        #'best_r2_epoch': best_r2_indx,\n",
    "        'syn_loss': syn_test_epoch_losses,\n",
    "        'ant_loss': ant_test_epoch_losses,\n",
    "        'label_loss': label_test_epoch_losses,\n",
    "        'test_losses': test_epoch_losses,        \n",
    "        'syn_acc': syn_test_epoch_accuracies,\n",
    "        'ant_acc': ant_test_epoch_accuracies,\n",
    "        'test_accs': test_epoch_accuracies,\n",
    "        #'pce_r2': pce_test_epoch_r2,\n",
    "        #'voc_r2': voc_test_epoch_r2,\n",
    "        #'jsc_r2': jsc_test_epoch_r2,\n",
    "        #'ff_r2': ff_test_epoch_r2,\n",
    "        #'test_r2s': test_epoch_r2s,\n",
    "        'train_syn_loss': syn_train_epoch_losses,\n",
    "        'train_ant_loss': ant_train_epoch_losses,\n",
    "        'train_label_loss': label_train_epoch_losses,\n",
    "    }\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting our graphs from the function to find the ideal lr and epoch\n",
    "def plot_fit_results(fit_dict):\n",
    "    lr = float(fit_dict['lr'])\n",
    "    best_loss_epoch = int(fit_dict['best_loss_epoch'])\n",
    "    best_acc_epoch = int(fit_dict['best_acc_epoch'])\n",
    "    #best_r2_epoch = int(fit_dict['best_r2_epoch'])\n",
    "    \n",
    "    test_loss = [float(i) for i in fit_dict['test_losses']]\n",
    "    syn_loss = [float(i) for i in fit_dict['syn_loss']]\n",
    "    ant_loss = [float(i) for i in fit_dict['ant_loss']]\n",
    "    label_loss = [float(i) for i in fit_dict['label_loss']]\n",
    "    \n",
    "    test_acc = [float(i) for i in fit_dict['test_accs']]\n",
    "    syn_acc = [float(i) for i in fit_dict['syn_acc']]\n",
    "    ant_acc = [float(i) for i in fit_dict['ant_acc']]\n",
    "    \n",
    "    #test_r2 = [float(i) for i in fit_dict['test_r2s']]\n",
    "    #pce_r2 = [float(i) for i in fit_dict['pce_r2']]\n",
    "    #voc_r2 = [float(i) for i in fit_dict['voc_r2']]\n",
    "    #jsc_r2 = [float(i) for i in fit_dict['jsc_r2']]\n",
    "    #ff_r2 = [float(i) for i in fit_dict['ff_r2']]\n",
    "    \n",
    "    train_syn_loss = [float(i) for i in fit_dict['train_syn_loss']]\n",
    "    train_ant_loss = [float(i) for i in fit_dict['train_ant_loss']]\n",
    "    train_label_loss = [float(i) for i in fit_dict['train_label_loss']]\n",
    "\n",
    "    epochs = np.arange(0, (len(test_loss)), 1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 6))\n",
    "    ax1.plot(epochs, pce_loss, c = 'r', label = 'syn loss')\n",
    "    ax1.plot(epochs, voc_loss, c = 'g', label = 'ant loss')\n",
    "    ax1.plot(epochs, jsc_loss, c = 'b', label = 'label loss')\n",
    "    ax1.plot(epochs, test_loss, c = 'c', label = 'total loss')\n",
    "    ax1.plot(epochs, train_pce_loss, c = 'r', linestyle = '-.', label = 'syn train loss')\n",
    "    ax1.plot(epochs, train_voc_loss, c = 'g', linestyle = '-.', label = 'ant train loss')\n",
    "    ax1.plot(epochs, train_jsc_loss, c = 'b', linestyle = '-.', label = 'label train loss')\n",
    "    ax1.scatter(best_loss_epoch, min(test_loss), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Custom Error Loss')\n",
    "    ax1.legend(loc = 'best')\n",
    "    ax1.set_title(f'Custom Loss with lr = {lr}')\n",
    "\n",
    "    ax2.plot(epochs, pce_acc, c = 'r', label = 'syn acc')\n",
    "    ax2.plot(epochs, voc_acc, c = 'g', label = 'ant acc')\n",
    "    ax2.plot(epochs, test_acc, c = 'b', label = 'total acc')\n",
    "    ax2.scatter(best_acc_epoch, min(test_acc), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Mean Absolute Percent Accuracy')\n",
    "    ax2.legend(loc = 'best')\n",
    "    ax2.set_title(f'Accuracies with lr = {lr}')\n",
    "\n",
    "    #ax3.plot(epochs, pce_r2, c = 'r', label = 'pce R$^2$')\n",
    "    #ax3.plot(epochs, voc_r2, c = 'g', label = 'voc R$^2$')\n",
    "    #ax3.plot(epochs, jsc_r2, c = 'b', label = 'jsc R$^2$')\n",
    "    #ax3.plot(epochs, ff_r2, c = 'c', label = 'ff R$^2$')\n",
    "    #ax3.plot(epochs, test_r2, c = 'k', label = 'total R$^2$')\n",
    "    #ax3.scatter(best_r2_epoch, max(test_r2), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    #ax3.set_xlabel('Epochs')\n",
    "    #ax3.set_ylabel('R$^2$')\n",
    "    #ax3.legend(loc = 'best')\n",
    "    #ax3.set_title(f'R$^2$ with lr = {lr}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Utilities\n",
    "\n",
    "def init_weights(model):\n",
    "    \n",
    "    classname = model.__class__.__name__\n",
    "    \n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.zeros_(model.bias)\n",
    "        \n",
    "    elif classname.find('Conv2d') != -1:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.zeros_(model.bias)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.zeros_(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def plot_error_accuracy\n",
    " #   fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "  #  epochs = np.arange(1, (num_epochs+1), 1)\n",
    "#\n",
    " #   plt.plot(epochs, train_epoch_losses, c = 'k', label = 'training error')\n",
    "  ## plt.legend(loc = 'upper right')\n",
    "    #plt.title(\"Total Training & Testing Error\")\n",
    "    #ax.set_xlabel('Epoch')\n",
    "    #ax.set_ylabel('Total Custom Loss')\n",
    "    #plt.show()\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize = (8,6))\n",
    "    #plt.plot(epochs, syn_test_epoch_accuracies, c = 'k', label = 'syn accuracy')\n",
    "    #plt.plot(epochs, ant_test_epoch_accuracies, c = 'r', label = 'ant accuracy')\n",
    "    #plt.legend(loc = 'lower right')\n",
    "    #plt.title(\"Phase I Labeling Accuracy\")\n",
    "    #ax.set_xlabel('Epoch')\n",
    "    #ax.set_ylabel('Accuracy')\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
