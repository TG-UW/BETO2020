{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../Ant_Syn_Scraping/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import model_functions_PhaseI as functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture\n",
    "\n",
    "class Phase_I_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the first of two neural networks to be used to determine synonymy, antonymy or irrelevance.\n",
    "    Using w2v pre-trained embeddings that are then embedded into our NN using the nn.Embedding layer we are able to\n",
    "    obtain the encoded embeddings of two words (pushed as a tuple) in synonym and antonym subspaces. These encodings\n",
    "    are then used to calculate the synonymy and antonymy score of those two words.\n",
    "    This mimics the Distiller method described by Asif Ali et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dims, out_dims, common, model1):\n",
    "        super(Phase_I_NN, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedded = functions.Embedding_Pre_Trained_Weights(common, model1)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "        nn.Linear(50, 100), #expand\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100, 300),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.S_branch = nn.Sequential( #synonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300,100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.A_branch = nn.Sequential(#antonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300, 100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        #other option is to define activation function here i.e. self.Softplus = torch.nn.Softplus() and use it in the forward pass\n",
    "        \n",
    "        \n",
    "    def forward(self, index_tuple):\n",
    "        \n",
    "        em_1, em_2 = self.embedded(index_tuple)[0], self.embedded(index_tuple)[1]\n",
    "        \n",
    "        #pass through hidden layers. For each linear layer in the hidden/branches, use the activation function to push\n",
    "        out_w1 = self.hidden_layers(em_1) \n",
    "        out_w2 = self.hidden_layers(em_2)\n",
    "        \n",
    "        #pass each embedded data through each branch to be situated in subspaces\n",
    "        S1_out = self.S_branch(out_w1)\n",
    "        S2_out = self.S_branch(out_w2)\n",
    "        A1_out = self.A_branch(out_w1)\n",
    "        A2_out = self.A_branch(out_w2)\n",
    "        \n",
    "        #Need to find a way to collect encoder embeddings as well as their scoring\n",
    "            \n",
    "        synonymy_score = F.cosine_similarity(S1_out.view(1,-1),S2_out.view(1,-1), dim=1) #do these outside of the NN class\n",
    "        antonymy_score = torch.max(F.cosine_similarity(A1_out.view(1,-1),S2_out.view(1,-1)),F.cosine_similarity(A2_out.view(1,-1),S1_out.view(1,-1), dim=1))\n",
    "                              \n",
    "        #return synonymy_score, antonymy_score #the encoders in each subspace\n",
    "        \n",
    "        return S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
