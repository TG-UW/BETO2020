{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to create list of synonyms and antonyms from words found on Thesaurus.com and Word.net\n",
    "#First thought is to create a list of words of interest that can be queried directly by our script \n",
    "#Need to also create a list of list of the words that we used to create the syn-ant query list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Thesaurus.com (T.com)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling \"useful_words\" from the MacMillan Dictonary\n",
    "\n",
    "useful_words_MacMillan = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.macmillandictionary.com/us/thesaurus-category/american/describing-chemicals-and-chemical-processes')\n",
    "time.sleep(2)\n",
    "\n",
    "html_doc = driver.page_source # stores the source HTML code in the driver's page_source attribute\n",
    "   \n",
    "#words = re.findall(r'\"<h3>\"(\\S*?)\"</h3>\"', html_doc) \n",
    "\n",
    "soup = bs(html_doc)\n",
    "\n",
    "for a in soup.find_all('h3'):\n",
    "    useful_words_MacMillan.append(a.string)\n",
    "    \n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_set = ['corrosion', 'flammable', 'resistant', 'retardate', 'malleable', 'flexible', 'active', 'cm', 'mm',\n",
    "              'nm', 'angstrom','inhibitor', 'anticorrosive', 'impedance', 'steel', 'concentration', 'adsorption',\n",
    "              'absorption', 'surface', 'fe', 'polarization', 'percentage', 'studies', 'measurements', 'immersion',\n",
    "              'loss', 'acid', 'carbon', 'decontamination', 'efficiency', 'electrochemical', 'water', 'coatings',\n",
    "              'protection', 'penetration', 'al', 'oxidation', 'copper', 'metal', 'mol', 'rate', 'ph', 'brass',\n",
    "              'cathodic', 'environment', 'performance', 'elastic', 'constant', 'refractive', 'inductive', 'zinc',\n",
    "              'external', 'sodium', 'free', 'thermal', 'decomposition', 'organic', 'exposure', 'crystalline',\n",
    "              'protective', 'pb', 'lead', 'alkaline', 'asymmetric', 'additive', 'homogeneous', 'volatile',\n",
    "              'stainless', 'soil', 'reduction', 'sour', 'layer', 'negative', 'durable', 'autonomous', 'hazardous',\n",
    "              'absorb', 'similar', 'pattern', 'physical', 'activation', 'excess', 'transparent', 'synthesis',\n",
    "              'reactive', 'uniform', 'cohesive', 'conductive', 'hindrance', 'feedback', 'gaseous', 'barrier',\n",
    "              'aggregation', 'inclusion', 'microscopic', 'ethane', 'sulfur', 'cracking', 'mineral', 'stability', 'magnesium']\n",
    "sample_syn_list = []\n",
    "sample_ant_list = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for word in sample_set:\n",
    "    \n",
    "    driver.get('view-source:https://www.thesaurus.com/browse/' + word)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html_doc = driver.page_source # stores the source HTML code in the driver's page_source attribute\n",
    "   \n",
    "    #Issue with next two lines is that we are also picking up \"Related Terms\" synonyms and antonyms.\n",
    "    #Dirty fix is to keep only the 5 ant and syn results from a query after it has been cleaned.\n",
    "    syn = re.findall(r'\"similarity\":\"100\",\"isInformal\":\"0\",\"isVulgar\":null,\"term\":\"(\\S*?)\",\"targetTerm\"', html_doc) \n",
    "    ant = re.findall(r'\"similarity\":\"-100\",\"isInformal\":\"0\",\"isVulgar\":null,\"term\":\"(\\S*?)\",\"targetTerm\"', html_doc)\n",
    "    \n",
    "    sample_syn_list.append(syn)\n",
    "    sample_ant_list.append(ant)\n",
    "    \n",
    "    driver.refresh()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame creation based off amount of 'useful_words'\n",
    "sample_array = np.empty((len(sample_set),3))\n",
    "sample_array_df = pd.DataFrame(data=sample_array[0:,:], columns = {'Term', 'Synonyms', 'Antonyms'})\n",
    "\n",
    "#Creation of raw list of lists of syn/ant for each 'useful_word'\n",
    "raw_syn_list = [[] for i in range(len(sample_set))]\n",
    "raw_ant_list = [[] for i in range(len(sample_set))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up scraped data (remove repetitions, need to find out how many syn/ant we want to keep)\n",
    "#Populate raw list of lists with cleaned syn/ant for each'useful_word'\n",
    "for i in range(len(sample_set)):\n",
    "    for word in sample_syn_list[i]:\n",
    "        if word in raw_syn_list[i]:\n",
    "            continue\n",
    "        else:\n",
    "            raw_syn_list[i].append(word)\n",
    "    \n",
    "for i in range(len(sample_set)):\n",
    "    for word in sample_ant_list[i]:\n",
    "        if word in raw_ant_list[i]:\n",
    "            continue\n",
    "        else:\n",
    "            raw_ant_list[i].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_array_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00752536c24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Populate DataFrame with 'useful_word' and syn/ant for each 'useful_word'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#For each term we have a list of strings for its 'Antonyms' and 'Synonyms'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_array_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msample_array_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msample_array_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Synonyms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_syn_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_array_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Populate DataFrame with 'useful_word' and syn/ant for each 'useful_word'\n",
    "#For each term we have a list of strings for its 'Antonyms' and 'Synonyms'\n",
    "for i in range(len(sample_array_df)):\n",
    "    sample_array_df['Term'].loc[i] = sample_set[i]\n",
    "    sample_array_df['Synonyms'].loc[i] = raw_syn_list[i][:5]\n",
    "    sample_array_df['Antonyms'].loc[i] = raw_ant_list[i][:5]\n",
    "sample_array_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/Thomas/Desktop/BETO2020-Local/Ant_Syn_Scraping'\n",
    "os.chdir(data)\n",
    "sample_T_com_df = pd.read_csv('T.com_SynAntList.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(word):\n",
    "    word = word[1:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    for i in range(0,len(df)):\n",
    "        df.iloc[i,1] = df.iloc[i,1][1:-1] #Getting rid of the brackets around the tokens in Tokens column\n",
    "        df.iloc[i,2] = df.iloc[i,2][1:-1]\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        df.iloc[i,1] = df.iloc[i,1].split(', ') #Seperating each pair word with a ', '\n",
    "        df.iloc[i,2] = df.iloc[i,2].split(', ')\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        df.iloc[i,1] = list(map(remove_apostrophe, df.iloc[i,1])) #removine apostrophes around each word\n",
    "        df.iloc[i,2] = list(map(remove_apostrophe, df.iloc[i,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "sample_set = ['corrosion', 'flammable', 'resistant', 'retardate', 'malleable', 'flexible', 'active', 'cm', 'mm', 'nm', 'angstrom','inhibitor', 'anticorrosive', 'impedance', 'steel', 'concentration', 'adsorption', 'absorption', 'surface', 'fe', 'polarization', 'percentage', 'studies', 'measurements', 'immersion', 'loss', 'acid', 'carbon', 'decontamination', 'efficiency', 'electrochemical', 'water', 'coatings', 'protection', 'penetration', 'al', 'oxidation', 'copper', 'metal', 'mol', 'rate', 'ph', 'brass', 'cathodic', 'environment', 'performance', 'elastic', 'constant', 'refractive', 'inductive', 'zinc', 'external', 'sodium', 'free', 'thermal', 'decomposition', 'organic', 'exposure', 'crystalline', 'protective', 'pb', 'lead', 'alkaline', 'asymmetric', 'additive', 'homogeneous', 'volatile', 'stainless', 'soil', 'reduction', 'sour', 'layer', 'negative', 'durable', 'autonomous', 'hazardous', 'absorb', 'similar', 'pattern', 'physical', 'activation', 'excess', 'transparent', 'synthesis', 'reactive', 'uniform', 'cohesive', 'conductive', 'hindrance', 'feedback', 'gaseous', 'barrier', 'aggregation', 'inclusion', 'microscopic', 'ethane', 'sulfur', 'cracking', 'mineral', 'stability', 'magnesium']\n",
    "\n",
    "WN_syn_list = []\n",
    "WN_ant_list = []\n",
    "\n",
    "for word in sample_set:\n",
    "    \n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    \n",
    "    clean_syns = []\n",
    "    clean_ants = []\n",
    "\n",
    "    for word in range(len(set(synonyms))):\n",
    "        clean_syn = (list(set(synonyms)))[word].replace(\"_\", \" \")\n",
    "    \n",
    "        clean_syns.append(clean_syn)\n",
    "\n",
    "    for word in range(len(set(antonyms))):\n",
    "        clean_ant = (list(set(antonyms)))[word].replace(\"_\", \" \")\n",
    "    \n",
    "        clean_ants.append(clean_ant)\n",
    "        \n",
    "    WN_syn_list.append(clean_syns)\n",
    "    WN_ant_list.append(clean_ants)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding WN sourced words to the T.com df\n",
    "#Appending WN_syns or WN_ants to the \n",
    "for i in range(0,len(sample_T_com_df)):\n",
    "    if sample_T_com_df.iloc[i,1] == ['']:\n",
    "        sample_T_com_df.iloc[i,1] = WN_ant_list[i]\n",
    "    else:\n",
    "        continue\n",
    "    if sample_T_com_df.iloc[i,2] == ['']:\n",
    "        sample_T_com_df.iloc[i,2] = WN_syn_list[i]\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_WN_Tcom_df = sample_T_com_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_T_com_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(olddf): \n",
    "    master = []\n",
    "\n",
    "    for i in range(len(olddf)):\n",
    "        term = olddf.iloc[i,0]\n",
    "        antlist = olddf.iloc[i,1]\n",
    "        synlist = olddf.iloc[i,2]\n",
    "\n",
    "        if antlist is not []:\n",
    "            for ant in antlist:\n",
    "                master.append([term,ant,'ant'])\n",
    "\n",
    "        if synlist is not []:\n",
    "            for syn in synlist:\n",
    "                master.append([term,syn,'syn'])\n",
    "\n",
    "    return pd.DataFrame(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Thomas/Desktop/BETO2020-Local/Ant_Syn_Scraping/Sample_Refined_SynAntList.csv'\n",
    "df = pd.read_csv(path, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chemical entity normalization using pubchempy\n",
    "#Can normalize by: molecular_formula, iupac_name, and synonyms\n",
    "#Can even extract particular properties of the \n",
    "import pubchempy as pcp\n",
    "from mendeleev import element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_array = np.empty((110,3))\n",
    "pde_df = pd.DataFrame(data=pde_array[0:,:], columns = {'IUPAC','Element', 'Molecular Formula'})\n",
    "\n",
    "for i in range(1,110):\n",
    "    compound_ID = pcp.get_cids(element(i).name, 'name', list_return='flat')\n",
    "    compound = pcp.Compound.from_cid(compound_ID)\n",
    "    iupac = compound.iupac_name\n",
    "    formula = compound.molecular_formulaa\n",
    "    pde_df['Element'].iloc[i] = element(i).name\n",
    "    pde_df['IUPAC'].loc[i] = iupac\n",
    "    pde_df['Molecular Formula'].loc[i] = formula\n",
    "\n",
    "pde_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
