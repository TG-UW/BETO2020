{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec as wv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import math\n",
    "#import PhysicallyInformedLossFunction as PhysLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary from Carbon corpus and Word2Vec model trained on all abstracts\n",
    "#Opening contents of Word2Vec model1\n",
    "\n",
    "data = '/Users/Thomas/Desktop/BETO2020-Remote/Ant_Syn_Scraping/all_abstracts_model'\n",
    "os.chdir(data)\n",
    "model1 = wv.load('all_abstract_model.model')\n",
    "vocabulary1 = list(model1.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CombinedSynAntList.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f349a13b2427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/Thomas/Desktop/BETO2020-Local/Ant_Syn_Scraping/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CombinedSynAntList.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#data_df = data_df.rename(columns = {'Unnamed: 1':'word 1', 'Unnamed: 2':'word 2','Unnamed: 3':'relationship', 'Unnamed: 4': 'label'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CombinedSynAntList.xlsx'"
     ]
    }
   ],
   "source": [
    "data = '/Users/Thomas/Desktop/BETO2020-Remote/Ant_Syn_Scraping/'\n",
    "os.chdir(data)\n",
    "data_df = pd.read_excel('CombinedSynAntList.xlsx', index_col=0)\n",
    "#data_df = data_df.rename(columns = {'Unnamed: 1':'word 1', 'Unnamed: 2':'word 2','Unnamed: 3':'relationship', 'Unnamed: 4': 'label'})\n",
    "\n",
    "#Adding columns for the syn and ant score labeling as well as indices\n",
    "#replacing all NaN values with 0\n",
    "data_df['syn score'] = np.nan\n",
    "data_df['ant score'] = np.nan\n",
    "data_df['word 1 index'] = np.nan\n",
    "data_df['word 2 index'] = np.nan\n",
    "data_df = data_df.fillna(0)\n",
    "data_df = data_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(data_df['word 1'])\n",
    "list2 = list(data_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary1))) + list((set(list2).difference(vocabulary1)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "data_df = data_df[~data_df['word 1'].isin(missing)]\n",
    "data_df = data_df[~data_df['word 2'].isin(missing)]\n",
    "data_df = data_df[~data_df['label'].str.contains('#', na=False)]\n",
    "\n",
    "#reseting indeces after mask\n",
    "data_df.reset_index(inplace = True)\n",
    "\n",
    "#creating list of individual words that are both in vocabulary1 and excel sheet \n",
    "#dict.fromkeys() ensuring there are no duplicates\n",
    "common = list(set(list1)&set(vocabulary1))+list(set(list2)&set(vocabulary1))\n",
    "common = list(dict.fromkeys(common))\n",
    "common = sorted(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vocab list and call it in the Pre_Trained_Embeddings\n",
    "common_df = pd.DataFrame(common)\n",
    "common_df.to_csv('words_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = common_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Cancer\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_df..str.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration\n",
    "#creating list of weights to be embedded as pre_trained()\n",
    "#list of weights -> tensor to be fed in pre_trained() function\n",
    "for i in range(len(common)):\n",
    "    common[i] = model1.wv.__getitem__(common[i]).tolist()\n",
    "\n",
    "weight = torch.tensor(common)\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration, all embedded words' Tensors\n",
    "z = embedding(torch.LongTensor(input))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration, finding the Tensors for the words with indices 1 and 2\n",
    "input = (1,2)\n",
    "x, y = embedding(torch.LongTensor(input))[0], embedding(torch.LongTensor(input))[1]\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#adding the index of the words as they are positioned in the list where \"common\" is the list or individual words\n",
    "#creating data set with the w2v weights instead of strings\n",
    "#adding syn scores and ant scores for good syns and ants\n",
    "#anything not \"good\" has a score of 0\n",
    "\n",
    "for i in range(len(data_df)): \n",
    "    \n",
    "    data_df['word 1 index'].iloc[i] = common.index(data_df['word 1'].iloc[i])\n",
    "    data_df['word 2 index'].iloc[i] = common.index(data_df['word 2'].iloc[i])\n",
    "    \n",
    "    data_df['word 1'].iloc[i] = model1.wv.__getitem__(str(data_df['word 1'].iloc[i])).tolist()\n",
    "    data_df['word 2'].iloc[i] = model1.wv.__getitem__(str(data_df['word 2'].iloc[i])).tolist()\n",
    "    \n",
    "    if data_df['relationship'].iloc[i] == 'syn' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = 1\n",
    "        data_df['ant score'].iloc[i] = -1\n",
    "       \n",
    "    elif data_df['relationship'].iloc[i] == 'ant' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = -1 \n",
    "        data_df ['ant score'].iloc[i] = 1\n",
    "        \n",
    "    else:\n",
    "        data_df['syn score'].iloc[i] = 0  \n",
    "        data_df['ant score'].iloc[i] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.to_json('Phase_I_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_json('Phase_I_DATA.json', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our data into train and test dataframes\n",
    "\n",
    "X = data_df[['word 1 index', 'word 2 index']] #indices that will be used to find corresponding \"feature\" weights\n",
    "Y = data_df[['syn score', 'ant score']] #what will ultimately be predicted\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "w1_train = x_train['word 1 index']\n",
    "w1_test = x_test['word 1 index']\n",
    "w2_train = x_train['word 2 index']\n",
    "w2_test = x_test['word 2 index']\n",
    "ss_train = y_train['syn score']\n",
    "ss_test = y_test['syn score']\n",
    "as_train = y_train['ant score']\n",
    "as_test = y_test['ant score']\n",
    "\n",
    "train_data = {'word 1 index': w1_train, 'word 2 index': w2_train, 'syn score': ss_train, 'ant score': as_train}\n",
    "test_data = {'word 1 index': w1_test, 'word 2 index': w2_test, 'syn score': ss_test, 'ant score': as_test}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('Phase_I_Train.json')\n",
    "test_df.to_json('Phase_I_Test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "data_test = data_df\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list = list(zip(data_test['word 1 index'], data_test['word 2 index']))\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list_tensor = torch.tensor(test_list)\n",
    "test_list_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data, self.y_data\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   dataset = Phase_I_Train_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Test.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   dataset = Phase_I_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = Phase_I_Train_Dataset()\n",
    "test_data_set = Phase_I_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader cell here\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = train_data_set, batch_size = batch_size, shuffle = True)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = test_data_set, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Pre_Trained_Weights(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the pre-training of the Phase_I_NN neural network weights using a list of words from which\n",
    "    a list of weights can be obtained. It is then converted that can then be embedded using the from_pretrained() \n",
    "    function into the NN model\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(Embedding_Pre_Trained_Weights, self).__init__()\n",
    "    \n",
    "        words = pd.read_csv('words_list.csv')\n",
    "        \n",
    "        df['value'].str.get(0)\n",
    "        for i in range(len(words)):\n",
    "            words.iloc[i] = model.wv.__getitem__(words.iloc[i].str).tolist()\n",
    "    \n",
    "        weight = torch.tensor(words)\n",
    "    \n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "    \n",
    "    def forward(self, index):\n",
    "        \n",
    "        index_vector = self.embedding(torch.LongTensor(index))\n",
    "        \n",
    "        return index_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the first of two neural networks to be used to determine synonymy, antonymy or irrelevance.\n",
    "    Using w2v pre-trained embeddings that are then embedded into our NN using the nn.Embedding layer we are able to\n",
    "    obtain the encoded embeddings of two words (pushed as a tuple) in synonym and antonym subspaces. These encodings\n",
    "    are then used to calculate the synonymy and antonymy score of those two words.\n",
    "    This mimics the Distiller method described by Asif Ali et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dims, out_dims, common, model1):\n",
    "        super(Phase_I_NN, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedded = Embedding_Pre_Trained_Weights(common, model1)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "        nn.Linear(50, 100), #expand\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100, 300),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.S_branch = nn.Sequential( #synonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300,100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.A_branch = nn.Sequential(#antonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300, 100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        #other option is to define activation function here i.e. self.Softplus = torch.nn.Softplus() and use it in the forward pass\n",
    "        \n",
    "        \n",
    "    def forward(self, index_tuple):\n",
    "        \n",
    "        em_1, em_2 = self.embedded(index_tuple)[0], self.embedded(index_tuple)[1]\n",
    "        \n",
    "        #pass through hidden layers. For each linear layer in the hidden/branches, use the activation function to push\n",
    "        out_w1 = self.hidden_layers(em_1) \n",
    "        out_w2 = self.hidden_layers(em_2)\n",
    "        \n",
    "        #pass each embedded data through each branch to be situated in subspaces\n",
    "        S1_out = self.S_branch(out_w1)\n",
    "        S2_out = self.S_branch(out_w2)\n",
    "        A1_out = self.A_branch(out_w1)\n",
    "        A2_out = self.A_branch(out_w2)\n",
    "        \n",
    "        #Need to find a way to collect encoder embeddings as well as their scoring\n",
    "            \n",
    "        synonymy_score = F.cosine_similarity(S1_out.view(1,-1),S2_out.view(1,-1), dim=1) #do these outside of the NN class\n",
    "        antonymy_score = torch.max(F.cosine_similarity(A1_out.view(1,-1),S2_out.view(1,-1)),F.cosine_similarity(A2_out.view(1,-1),S1_out.view(1,-1), dim=1))\n",
    "                              \n",
    "        #return synonymy_score, antonymy_score #the encoders in each subspace\n",
    "        \n",
    "        return S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Synonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the synonym subspace. A lower and higher bound for synonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"  \n",
    "    def __init__ (self):\n",
    "        super(Loss_Synonymy, self).__init__()\n",
    "        \n",
    "    def forward(self, S1_out, S2_out, synonymy_score):\n",
    "        \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "            \n",
    "        for x, a, b in zip(synonymy_score, S1_out, S2_out): #x=synonymy_score, a=S1_out, b=S2_out\n",
    "    \n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2))))) #assumed Euclidean Distance\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2,error)\n",
    "                \n",
    "        \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Antonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the antonym subspace. A lower and higher bound for antonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Loss_Antonymy, self).__init__()\n",
    "       \n",
    "    def forward(self, S2_out, A1_out, antonymy_score): \n",
    "        \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "    \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "        \n",
    "        for x, a, b in zip(antonymy_score, A1_out, S2_out): #x=antonymy_score, a=A1_out, b=S2_out (to ensure trans-transitivity)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2)))))\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2, error)\n",
    "                 \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        error_total.retain_grad()\n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Labels(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is the last portion of the general loss function. Here the predicted synonymy and antonymy scores\n",
    "    are concatenated and compared to the concatenated labeled synonymy and antonymy scores\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss_Labels, self).__init__()\n",
    "       \n",
    "    def forward(self, S1_out, synonymy_score, antonymy_score):\n",
    "        \n",
    "        result_list = torch.zeros(S1_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        for x, y in zip(synonymy_score, antonymy_score):\n",
    "            \n",
    "            #print(synonymy_score)\n",
    "            #print(antonymy_score)\n",
    "            error = torch.nn.functional.log_softmax(torch.stack((x,y),dim=0), dim=0)\n",
    "            error = torch.argmax(error).float()\n",
    "            #error.squeeze()\n",
    "            \n",
    "            error.requires_grad = True\n",
    "            error.retain_grad()\n",
    "            result_list[element_count] = error\n",
    "            element_count += 1\n",
    "            result = torch.neg(result_list.mean())\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_train_model(model, training_data_set, optimizer):\n",
    "    \n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_train_losses = []\n",
    "    label_train_losses = []\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss = []\n",
    "    label_train_epoch_loss = []\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy()\n",
    "    ant_criterion = Loss_Antonymy()\n",
    "    label_criterion = Loss_Labels()\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        \n",
    "        S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "        ant_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "        label_loss = label_criterion(S1_out, synonymy_score, antonymy_score)\n",
    "        total_loss = syn_loss + ant_loss + label_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss, ant_loss, label_loss]) #total_loss]) #This may be where we want to add the L_M part of the loss function\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        ant_train_losses.append(ant_loss.item())\n",
    "        label_train_losses.append(label_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    ant_train_epoch_loss.append(sum(ant_train_losses)/train_total)\n",
    "    label_train_epoch_loss.append(sum(label_train_losses)/train_total)\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy()\n",
    "    ant_criterion = Loss_Antonymy()\n",
    "    label_criterion = Loss_Labels()\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        ant_test_losses = []\n",
    "        label_test_losses = []\n",
    "        \n",
    "        syn_test_acc_list = []\n",
    "        ant_test_acc_list = []\n",
    "        label_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(inputs)\n",
    "\n",
    "            #calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "            ant_test_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "            label_test_loss = label_criterion(S1_out, synonymy_score, antonymy_score)\n",
    "            \n",
    "            test_loss = syn_test_loss + ant_test_loss + label_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            ant_test_losses.append(ant_test_loss.item())\n",
    "            label_test_losses.append(label_test_loss.item())\n",
    "            test_total += 1 \n",
    "            \n",
    "            #accuracy function\n",
    "            syn_el_count = 0\n",
    "            syn_correct = 0\n",
    "            for x, y in zip(synonymy_score, labels[0]):\n",
    "                if x*0.8 <= y:\n",
    "                    syn_correct += 1\n",
    "                    syn_el_count += 1\n",
    "\n",
    "                else:\n",
    "                    syn_el_count += 1\n",
    "            \n",
    "            ant_el_count = 0\n",
    "            ant_correct = 0\n",
    "            \n",
    "            for x, y in zip(antonymy_score, labels[1]):\n",
    "                if x*0.8 <= y:\n",
    "                    ant_correct += 1\n",
    "                    ant_el_count += 1\n",
    "\n",
    "                else:\n",
    "                    ant_el_count += 1\n",
    "            \n",
    "        syn_acc = (syn_correct/syn_el_count) * 100\n",
    "        syn_test_acc_list.append(syn_acc)\n",
    "        \n",
    "        ant_acc = (ant_correct/ant_el_count) * 100\n",
    "        ant_test_acc_list.append(ant_acc)\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        ant_test_epoch_loss = sum(ant_test_losses)/test_total\n",
    "        label_test_epoch_loss = sum(label_test_losses)/test_total\n",
    "        \n",
    "        syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "        ant_epoch_acc = sum(ant_test_acc_list)/test_total\n",
    "\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        print(f\"Total Epoch Antonym Testing Accuracy is: {ant_epoch_acc}\")\n",
    "        print(f\"Total Epoch Synonym Testing Accuracy is: {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0a0858f17996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# takes in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# predicts synonymy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhase_I_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#define the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8b85944059b6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dims, out_dims, common, model1)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding_Pre_Trained_Weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8dec4f9cc14b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words_list.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = Phase_I_NN(in_dims = 50, out_dims = 2, common = common, model1 = model1).to(device)\n",
    "model.apply(nuts.init_weights)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, amsgrad = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "ant_train_epoch_losses = []\n",
    "label_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "ant_test_epoch_losses = []\n",
    "label_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "ant_test_epoch_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss  = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "    label_train_epoch_losses.append(label_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "    label_test_epoch_losses.append(label_test_epoch_loss)\n",
    "    \n",
    "    syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "    ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "    #test_epoch_losses.append(test_epoch_loss)\n",
    "    #syn_test_epoch_losses.append(syn_test_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "def fit(model, lr, epochs):\n",
    "\n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    #empty list to hold loss per epoch\n",
    "    train_epoch_losses = []\n",
    "    syn_train_epoch_losses = []\n",
    "    ant_train_epoch_losses = []\n",
    "    label_train_epoch_losses = []\n",
    "\n",
    "    test_epoch_losses = []\n",
    "    syn_test_epoch_losses = []\n",
    "    ant_test_epoch_losses = []\n",
    "    label_off_test_epoch_losses = []\n",
    "\n",
    "\n",
    "    syn_test_epoch_accuracies = []\n",
    "    ant_test_epoch_accuracies = []\n",
    "\n",
    "    #pce_test_epoch_r2 = []\n",
    "    #voc_test_epoch_r2 = []\n",
    "    #jsc_test_epoch_r2 = []\n",
    "    #ff_test_epoch_r2 = []\n",
    "    #test_epoch_r2s = []\n",
    "\n",
    "    save_epochs = np.arange(0, num_epochs, 5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('On epoch ', epoch)\n",
    "    \n",
    "        #save_dir = \"/Users/Thomas/Desktop/BETO2020-Remote/Ant_Syn_Scraping/\"\n",
    "        #model_name = \"Phase_I_II_NN\"\n",
    "        #model_path = save_dir+model_name+'*.pt'\n",
    "        #if epoch < 10:\n",
    "            #save_path = save_dir + model_name + '_epoch0' + str(epoch) + '.pt'\n",
    "        #else:\n",
    "            #save_path = save_dir + model_name + '_epoch' + str(epoch) + '.pt'\n",
    "        \n",
    "#     if glob.glob(model_path) != []:\n",
    "#         model_states = glob.glob(model_path)\n",
    "#         model_states = sorted(model_states)\n",
    "#         previous_model = model_states[-1]\n",
    "        \n",
    "#         model, optimizer = nuts.load_trained_model(previous_model, model, optimizer)\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "    label_train_epoch_losses.append(label_train_epoch_loss)\n",
    "    \n",
    "        \n",
    "    test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set)\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_loss)\n",
    "    ant_test_epoch_losses.append(ant_test_loss)\n",
    "    label_test_epoch_losses.append(label_test_loss)\n",
    "    \n",
    "    #tot_tst_loss = sum(test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss)\n",
    "    #test_epoch_losses.append(tot_tst_loss)\n",
    "    \n",
    "    syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "    ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "    \n",
    "    tot_tst_acc = sum(syn_epoch_acc, ant_epoch_acc)\n",
    "    test_epoch_accuracies.append(tot_tst_acc)\n",
    "    \n",
    "    print('Finished epoch ', epoch)\n",
    "        \n",
    "    best_loss_indx = test_epoch_losses.index(min(test_epoch_losses))\n",
    "    best_acc_indx = test_epoch_accuracies.index(min(test_epoch_accuracies))\n",
    "    \n",
    "    fit_results = {\n",
    "        'lr': lr,\n",
    "        'best_loss_epoch': best_loss_indx,\n",
    "        'best_acc_epoch': best_acc_indx,\n",
    "        #'best_r2_epoch': best_r2_indx,\n",
    "        'syn_loss': syn_test_epoch_losses,\n",
    "        'ant_loss': ant_test_epoch_losses,\n",
    "        'label_loss': label_test_epoch_losses,\n",
    "        'test_losses': test_epoch_losses,        \n",
    "        'syn_acc': syn_test_epoch_accuracies,\n",
    "        'ant_acc': ant_test_epoch_accuracies,\n",
    "        'test_accs': test_epoch_accuracies,\n",
    "        #'pce_r2': pce_test_epoch_r2,\n",
    "        #'voc_r2': voc_test_epoch_r2,\n",
    "        #'jsc_r2': jsc_test_epoch_r2,\n",
    "        #'ff_r2': ff_test_epoch_r2,\n",
    "        #'test_r2s': test_epoch_r2s,\n",
    "        'train_syn_loss': syn_train_epoch_losses,\n",
    "        'train_ant_loss': ant_train_epoch_losses,\n",
    "        'train_label_loss': label_train_epoch_losses,\n",
    "    }\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "lrs = np.linspace(1e-4, 1e-1, 150)\n",
    "\n",
    "lr_opt = {}\n",
    "\n",
    "for i, lr in enumerate(lrs):\n",
    "    print(f'  optimization loop {i}')\n",
    "    print('-----------------------------')\n",
    "    \n",
    "    lr_opt[i] = fit(model, lr, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/20200715_OPVNN2_hpo_results_r2.json', 'w') as fp:\n",
    "    json.dump(lr_opt, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/20200715_OPVNN2_hpo_results_r2.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_results(fit_dict):\n",
    "    lr = float(fit_dict['lr'])\n",
    "    best_loss_epoch = int(fit_dict['best_loss_epoch'])\n",
    "    best_acc_epoch = int(fit_dict['best_acc_epoch'])\n",
    "    #best_r2_epoch = int(fit_dict['best_r2_epoch'])\n",
    "    \n",
    "    test_loss = [float(i) for i in fit_dict['test_losses']]\n",
    "    syn_loss = [float(i) for i in fit_dict['syn_loss']]\n",
    "    ant_loss = [float(i) for i in fit_dict['ant_loss']]\n",
    "    label_loss = [float(i) for i in fit_dict['label_loss']]\n",
    "    \n",
    "    test_acc = [float(i) for i in fit_dict['test_accs']]\n",
    "    syn_acc = [float(i) for i in fit_dict['syn_acc']]\n",
    "    ant_acc = [float(i) for i in fit_dict['ant_acc']]\n",
    "    \n",
    "    #test_r2 = [float(i) for i in fit_dict['test_r2s']]\n",
    "    #pce_r2 = [float(i) for i in fit_dict['pce_r2']]\n",
    "    #voc_r2 = [float(i) for i in fit_dict['voc_r2']]\n",
    "    #jsc_r2 = [float(i) for i in fit_dict['jsc_r2']]\n",
    "    #ff_r2 = [float(i) for i in fit_dict['ff_r2']]\n",
    "    \n",
    "    train_syn_loss = [float(i) for i in fit_dict['train_syn_loss']]\n",
    "    train_ant_loss = [float(i) for i in fit_dict['train_ant_loss']]\n",
    "    train_label_loss = [float(i) for i in fit_dict['train_label_loss']]\n",
    "\n",
    "    epochs = np.arange(0, (len(test_loss)), 1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 6))\n",
    "    ax1.plot(epochs, pce_loss, c = 'r', label = 'syn loss')\n",
    "    ax1.plot(epochs, voc_loss, c = 'g', label = 'ant loss')\n",
    "    ax1.plot(epochs, jsc_loss, c = 'b', label = 'label loss')\n",
    "    ax1.plot(epochs, test_loss, c = 'c', label = 'total loss')\n",
    "    ax1.plot(epochs, train_pce_loss, c = 'r', linestyle = '-.', label = 'syn train loss')\n",
    "    ax1.plot(epochs, train_voc_loss, c = 'g', linestyle = '-.', label = 'ant train loss')\n",
    "    ax1.plot(epochs, train_jsc_loss, c = 'b', linestyle = '-.', label = 'label train loss')\n",
    "    ax1.scatter(best_loss_epoch, min(test_loss), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Custom Error Loss')\n",
    "    ax1.legend(loc = 'best')\n",
    "    ax1.set_title(f'Custom Loss with lr = {lr}')\n",
    "\n",
    "    ax2.plot(epochs, pce_acc, c = 'r', label = 'syn acc')\n",
    "    ax2.plot(epochs, voc_acc, c = 'g', label = 'ant acc')\n",
    "    ax2.plot(epochs, test_acc, c = 'b', label = 'total acc')\n",
    "    ax2.scatter(best_acc_epoch, min(test_acc), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Mean Absolute Percent Accuracy')\n",
    "    ax2.legend(loc = 'best')\n",
    "    ax2.set_title(f'Accuracies with lr = {lr}')\n",
    "\n",
    "    #ax3.plot(epochs, pce_r2, c = 'r', label = 'pce R$^2$')\n",
    "    #ax3.plot(epochs, voc_r2, c = 'g', label = 'voc R$^2$')\n",
    "    #ax3.plot(epochs, jsc_r2, c = 'b', label = 'jsc R$^2$')\n",
    "    #ax3.plot(epochs, ff_r2, c = 'c', label = 'ff R$^2$')\n",
    "    #ax3.plot(epochs, test_r2, c = 'k', label = 'total R$^2$')\n",
    "    #ax3.scatter(best_r2_epoch, max(test_r2), alpha = 0.8, s = 64, c = 'turquoise')\n",
    "    #ax3.set_xlabel('Epochs')\n",
    "    #ax3.set_ylabel('R$^2$')\n",
    "    #ax3.legend(loc = 'best')\n",
    "    #ax3.set_title(f'R$^2$ with lr = {lr}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, fit_dict in data.items():\n",
    "    plot_fit_results(fit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete after plots are confirmed to work\n",
    "def best_fit_lrs(fit_dict):\n",
    "    \"\"\"\n",
    "    Function that sorts through hyperparameter optimization results dictionaries and collects\n",
    "    the 5 best learning rates and epochs.\n",
    "    \"\"\"\n",
    "    best_params = {}\n",
    "    \n",
    "    #initialize variables with poor values\n",
    "    lowest_loss = 200\n",
    "    lowest_acc = 200\n",
    "    #highest_r2 = -200\n",
    "    \n",
    "    #for each lr fit, collect best values\n",
    "    best_losses = []\n",
    "    best_accs = []\n",
    "    best_r2s = []\n",
    "    \n",
    "    for key, fit in fit_dict.items():\n",
    "        \n",
    "        loss_ep = fit['best_loss_epoch']\n",
    "#         acc_ep = fit['best_acc_epoch']\n",
    "#         r2_ep = fit['best_r2_epoch']\n",
    "        \n",
    "        #these all need to come from the same epoch for parameter selection\n",
    "        best_losses.append(fit['test_losses'][loss_ep])\n",
    "        best_accs.append(fit['test_accs'][loss_ep])\n",
    "        #best_r2s.append(fit['test_r2s'][loss_ep])\n",
    "        \n",
    "    return\n",
    "\n",
    "def plot_best_fit_lrs(fit_dict):\n",
    "    \"\"\"\n",
    "    A function that plots the best loss, accuracy, and r2 of a lr fit series. Note\n",
    "    that the epoch shown is loss's best epoch, which many not correspond to the best\n",
    "    epoch for accuracy or r2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #for each lr fit, collect best values\n",
    "    lrs = []\n",
    "    best_losses = []\n",
    "    best_accs = []\n",
    "    best_r2s = []\n",
    "    \n",
    "    for key, fit in fit_dict.items():\n",
    "        lrs.append(fit['lr'])\n",
    "        \n",
    "        loss_ep = fit['best_loss_epoch']\n",
    "        acc_ep = fit['best_acc_epoch']\n",
    "        r2_ep = fit['best_r2_epoch']\n",
    "        \n",
    "        #these will all need to come from the same epoch during parameter selection\n",
    "        best_losses.append(fit['test_losses'][loss_ep])\n",
    "        best_accs.append(fit['test_accs'][acc_ep])\n",
    "        best_r2s.append(fit['test_r2s'][r2_ep])\n",
    "        \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 6))\n",
    "    ax1.plot(lrs, best_losses, c = 'r')\n",
    "    ax1.scatter(lrs[best_losses.index(min(best_losses))], min(best_losses), s = 64, alpha = 0.8, c = 'turquoise')\n",
    "    ax1.set_xlabel('Learning Rates')\n",
    "    ax1.set_ylabel('Mean Squared Error Loss')\n",
    "    ax1.set_title(f'MSE Loss with lr')\n",
    "    \n",
    "    ax2.plot(lrs, best_accs, c = 'r')\n",
    "    ax2.scatter(lrs[best_accs.index(min(best_accs))], min(best_accs), s = 64, alpha = 0.8, c = 'turquoise')\n",
    "    ax2.set_xlabel('Learning Rates')\n",
    "    ax2.set_ylabel('Mean Absolute Percent Error')\n",
    "    ax2.set_title(f'MAPE with lr')\n",
    "    \n",
    "    ax3.plot(lrs, best_r2s, c = 'r')\n",
    "    ax3.scatter(lrs[best_r2s.index(max(best_r2s))], max(best_r2s), s = 64, alpha = 0.8, c = 'turquoise')\n",
    "    ax3.set_xlabel('Learning Rates')\n",
    "    ax3.set_ylabel('R$^2$')\n",
    "    ax3.set_title(f'R$^2$ with lr')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_fit_lrs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_trian_losses\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    ant_criterion = \n",
    "    \n",
    "    #for features, labels in training_data_set: \n",
    "    \n",
    "    #may need to change above \"features\" portion here to accomodate for our custom dataset\n",
    "    #below is proposed alternative\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        syn_out = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(syn_out, labels)\n",
    "        total_loss = syn_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss is: {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        #syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            syn_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(syn_out, labels)\n",
    "            \n",
    "            test_loss = syn_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "            #syn_acc = accuracy(syn_out)\n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        \n",
    "        #syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, #syn_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = SYN_TEST(in_dims = 50, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #syn_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
