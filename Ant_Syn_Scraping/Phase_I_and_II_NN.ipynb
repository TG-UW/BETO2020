{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec as wv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import math\n",
    "#import PhysicallyInformedLossFunction as PhysLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary from Carbon corpus and Word2Vec model trained on all abstracts\n",
    "#Opening contents of Word2Vec model1\n",
    "data = '/Users/Thomas/Desktop/BETO2020-master/Ant_Syn_Scraping/all_abstracts_model'\n",
    "os.chdir(data)\n",
    "model1 = wv.load('all_abstract_model.model')\n",
    "vocabulary1 = list(model1.wv.vocab)\n",
    "#use model.build_vocab(sentence, update=True) to add missing words to model's vocabulary?\n",
    "#or delete the rows that yield the KeyError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/Thomas/Desktop/BETO2020-master/Ant_Syn_Scraping/'\n",
    "os.chdir(data)\n",
    "data_df = pd.read_excel('Carbon_SynAntList_Full_Refined_copy.xlsx', skip_rows=1, nrows=2000, index_col=0)\n",
    "data_df = data_df.rename(columns = {'Unnamed: 1':'word 1', 'Unnamed: 2':'word 2','Unnamed: 3':'relationship', 'Unnamed: 4': 'label'})\n",
    "#Adding columns for the syn and ant score labeling\n",
    "data_df['syn score'] = np.nan\n",
    "data_df['ant score'] = np.nan\n",
    "data_df = data_df.fillna(0)\n",
    "data_df = data_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(data_df['word 1'])\n",
    "list2 = list(data_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary1))) + list((set(list2).difference(vocabulary1)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "data_df = data_df[~data_df['word 1'].isin(missing)]\n",
    "data_df = data_df[~data_df['word 2'].isin(missing)]\n",
    "\n",
    "#reseting indeces after mask\n",
    "data_df.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_df)): \n",
    "    data_df['word 1'].iloc[i] = model1.wv.__getitem__(str(data_df['word 1'].iloc[i])).tolist()\n",
    "    data_df['word 2'].iloc[i] = model1.wv.__getitem__(str(data_df['word 2'].iloc[i])).tolist()\n",
    "    \n",
    "    if data_df['relationship'].iloc[i] == 'syn' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = 1\n",
    "        data_df['ant score'].iloc[i] = -1\n",
    "       \n",
    "    elif data_df['relationship'].iloc[i] == 'ant' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = -1 \n",
    "        data_df ['ant score'].iloc[i] = 1\n",
    "        \n",
    "    else:\n",
    "        data_df['syn score'].iloc[i] = 0  \n",
    "        data_df['ant score'].iloc[i] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_json('Phase_I_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('Phase_I_DATA.json', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df[['word 1', 'word 2']]\n",
    "Y = data_df[['syn score', 'ant score']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "w1_train = x_train['word 1']\n",
    "w1_test = x_test['word 1']\n",
    "w2_train = x_train['word 2']\n",
    "w2_test = x_test['word 2']\n",
    "ss_train = y_train['syn score']\n",
    "ss_test = y_test['syn score']\n",
    "as_train = y_train['ant score']\n",
    "as_test = y_test['ant score']\n",
    "\n",
    "train_data = {'word 1': w1_train, 'word 2': w2_train, 'syn score': ss_train, 'ant score': as_train}\n",
    "test_data = {'word 1': w1_test, 'word 2': w2_test, 'syn score': ss_test, 'ant score': as_test}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('Phase_I_Train.json')\n",
    "test_df.to_json('Phase_I_Test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "data_test = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list = list(zip(data_test['word 1'], data_test['word 2']))\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list_tensor = torch.tensor(test_list)\n",
    "test_list_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "data_test = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "data_test.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1'], data_test['word 2'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data, self.y_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Phase_I_Train_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Test.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1'], data_test['word 2'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Phase_I_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader cell here\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = Phase_I_Train_Dataset(), batch_size = batch_size, shuffle = True)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = Phase_I_Test_Dataset(), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the first of two neural networks to be used to determine synonymy, antonymy or irrelevance.\n",
    "    Using w2v pre-trained embeddings that are then embedded into our NN using the nn.Embedding layer we are able to\n",
    "    obtain the encoded embeddings of two words (pushed as a tuple) in synonym and antonym subspaces. These encodings\n",
    "    are then used to calculate the synonymy and antonymy score of those two words.\n",
    "    This mimics the Distiller method described by Asif Ali et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super(SYN_TEST, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.nn.Embedding(2,in_dims)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "        nn.Linear(50, 100), #expand\n",
    "        nn.Softplus(),\n",
    "        snn.Linear(100, 300),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.S_branch = nn.Sequential( #synonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300,100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus(),\n",
    "        #nn.Linear(50,1), #included output layer\n",
    "        #nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.A_branch = nn.Sequential(#need some activation function after each Linear function. Softmax is the NLP convention.\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300, 100) #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus(),\n",
    "        #nn.Linear(50,1), #included output layer\n",
    "        #nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        #other option is to define activation function here i.e. self.Softplus = torch.nn.Softplus() and use it in the forward pass\n",
    "        \n",
    "        #still need an output layer to be doing the syn and ant score predictions?\n",
    "        \n",
    "    def forward(self, (w1, w2)):\n",
    "    \n",
    "        em_w1, em_w2 = nn.Embedding((w1,w2))\n",
    "        \n",
    "        #pass through hidden layers. For each linear layer in the hidden/branches, use the activation function to push\n",
    "        out_w1 = self.hidden_layers(em_w1) \n",
    "        out_w2 = self.hidden_layers(em_w2)\n",
    "        \n",
    "        #pass each embedded data through each branch to be situated in subspaces\n",
    "        S1_out = self.S_branch(out_w1)\n",
    "        S2_out = self.S_branch(out_w2)\n",
    "        A1_out = self.A_branch(out_w1)\n",
    "        A2_out = self.A_branch(out_w2)\n",
    "        \n",
    "        #Need to find a way to collect encoder embeddings as well as their scoring\n",
    "            \n",
    "        synonymy_score = F.cosine_similarity(S1_out,S2_out) #do these outside of the NN class\n",
    "        antonymy_score = torch.max(F.cosine_similarity(A1_out,S2_out),F.cosine_similarity(A2_out,S1_out))\n",
    "                              \n",
    "        #return synonymy_score, antonymy_score #the encoders in each subspace\n",
    "        \n",
    "        return S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Synonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the synonym subspace. A lower and higher bound for synonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "     #will need to include S score here\n",
    "    \n",
    "    def __init__ (self, lower, upper):\n",
    "        super(Loss_Synonymy, self).__init__()\n",
    "        self.lower = lower #-1\n",
    "        self.upper = upper #1\n",
    "    \n",
    "    def forward(self, predictions, labels):\n",
    "        \n",
    "        result_list = torch.zeros(predictions.size(0))\n",
    "        element_count = 0\n",
    "       \n",
    "        for x, y in zip(predictions[4], labels[0]): #use F.relu\n",
    "            \n",
    "            #error_1 is the first part of L_s\n",
    "            error_1 = torch.zeros()\n",
    "            \n",
    "            error_1 = F.relu(torch.add(1, torch.neg(torch.tanh((S1_out, S2_out)))))\n",
    "            \n",
    "            #error_2 is the second part of L_s\n",
    "            error_2 = \n",
    "            \n",
    "            \n",
    "            \n",
    "            error_total = torch.add(error_1, error_2)\n",
    "            \n",
    "            return error_total\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Antonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the antonym subspace. A lower and higher bound for antonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lower, upper):\n",
    "        super(Loss_Antonymy, self).__init__()\n",
    "        self.lower = lower #-1\n",
    "        self.upper = upper #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_train_model(model, Phase_I_training_data_set, optimizer)\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_train_losses\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss =[]\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy(?)\n",
    "    ant_criterion = Loss_Antonymy(?)\n",
    "    \n",
    "    for i, data in enumerate(Phase_I_training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        synonymy_score, antonymy_score = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(synonymy_score, label[0])\n",
    "        ant_loss = ant_criterion(antonymy_score, label[1])\n",
    "        total_loss = syn_loss + ant_loss #+ L_M #need to create a total_loss_criterion\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss, ant_loss, #total_loss]) #This may be where we want to add the L_M part of the loss function\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        ant_train_losses.append(ant_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    ant_train_epoch_loss.append(sum(ant_train_losses)/train_total)\n",
    "    \n",
    "    return model, train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = S_loss(?) #? is where we will establish threshold\n",
    "    ant_criterion = A_loss(?)\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        ant_test_losses = []\n",
    "        \n",
    "        syn_test_acc_list = []\n",
    "        ant_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            synonymy_score, antonymy_score = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(synonymy_score, labels[0])\n",
    "            ant_test_loss = ant_criterion(antonymy_score, labels[1])\n",
    "            \n",
    "            test_loss = syn_test_loss + ant_test_loss #+L_M\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "            \n",
    "            syn_acc = accuracy(synonymy_score)\n",
    "            ant_acc = accuracy(antonymy_score)\n",
    "            \n",
    "            syn_test_acc_list.append(syn_acc.item())\n",
    "            ant_test_acc_list.append(ant_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        ant_test_epoch_loss = sum(ant_test_losses)/test_total\n",
    "        \n",
    "        syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "        ant_epoch_acc = sum(ant_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, syn_epoch_acc, ant_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = SYN_TEST(in_dims = 50, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "ant_train_epoches_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "ant_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "ant_test_epohc_accuracies\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #syn_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_trian_losses\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    ant_criterion = \n",
    "    \n",
    "    #for features, labels in training_data_set: \n",
    "    \n",
    "    #may need to change above \"features\" portion here to accomodate for our custom dataset\n",
    "    #below is proposed alternative\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        syn_out = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(syn_out, labels)\n",
    "        total_loss = syn_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss is: {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        #syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            syn_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(syn_out, labels)\n",
    "            \n",
    "            test_loss = syn_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "            #syn_acc = accuracy(syn_out)\n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        \n",
    "        #syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, #syn_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = SYN_TEST(in_dims = 50, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #syn_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
