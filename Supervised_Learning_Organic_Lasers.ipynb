{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def extractor(url,wait_time):\n",
    "    \"\"\"\n",
    "    Accepts a url and stores its html code before parsing and extracting the abstract and date as text.\n",
    "    Feeds directly into parser, so don't call this function unless you want to obtain the abstract\n",
    "    from a single url.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    #driver.find_element_by_id('').send_keys('')#\n",
    "    #driver.find_element_by_id ('').send_keys('')#\n",
    "    #driver.find_element_by_id('submit').click()#\n",
    "    time.sleep(wait_time) # important\n",
    "\n",
    "    html_doc = driver.page_source # stores the source HTML code in the driver's page_source attribute\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    abstract = soup.find('div', {'class':\"abstract author\"}).find('p').text\n",
    "    pub_date = soup.find('meta', {'name': \"citation_publication_date\"})['content']\n",
    "    \n",
    "    driver.quit()\n",
    "    return pub_date, abstract\n",
    "\n",
    "\n",
    "def parse_all(driver):\n",
    "    \"\"\"\n",
    "    The following method is designed to automatically parse each url contained in a long list \n",
    "    of scraped urls, and writes the title, abstract, and doi to a new text file with a user\n",
    "    input \"file_name.txt.\"\n",
    "    \"\"\"\n",
    "    url_lst = input(\"Enter name of file with .txt extension with list of urls: \")\n",
    "    data = pd.read_csv(url_lst,header=None,names=['url']) #text file containing a list of the scraped urls (should be in same directory)\n",
    "    file_name = input(\"Input the file name with .txt extension you wish to store abstracts in: \")\n",
    "    file = open(file_name,'w')\n",
    "\n",
    "    max_iters = len(data) #total number of scraped urls to be parsed\n",
    "    print(\"The parser will parse: \" + str(max_iters) + \" urls.\")\n",
    "\n",
    "    for i in range(0,max_iters):\n",
    "        print('On url ',i)\n",
    "        driver.refresh()\n",
    "        time.sleep(2)\n",
    "        urli = str(extractor(data.iloc[i,0],3))\n",
    "        file.write(urli)\n",
    "        file.write('\\n')\n",
    "    driver.quit()\n",
    "\n",
    "    return file_name\n",
    "\n",
    "### Actual executable code is shown below ###\n",
    "### driver = webdriver.Chrome()\n",
    "### parse_all(driver)\n",
    "\n",
    "def tokenizer(file_name):\n",
    "    \"\"\" \n",
    "    Accepts text file as a string (e.g. \"abstracts.txt\") containing a list of abstracts as input and cleans up text using regex.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        corpus = file.readlines()\n",
    "        processed_abstracts = [w.lower() for w in corpus]\n",
    "        processed_abstracts = [re.sub('[^a-zA-Z]', ' ', w) for w in processed_abstracts]\n",
    "        processed_abstracts = [re.sub(r'\\s+', ' ', w) for w in processed_abstracts]\n",
    "    tokens = [nltk.word_tokenize(sent) for sent in processed_abstracts]\n",
    "\n",
    "    for i in range(len(processed_abstracts)):\n",
    "        tokens[i] = [w for w in tokens[i] if w not in stopwords.words('english')]\n",
    "   \n",
    "    # Passes all tokens to Word2Vec to train model\n",
    "    model = Word2Vec(tokens, min_count=2)\n",
    "    print(model)\n",
    "    vocabulary = list(model.wv.vocab)\n",
    "\n",
    "    return model, vocabulary\n",
    "\n",
    "def single_abstract_tkzr(abstract):\n",
    "    \"\"\"\n",
    "    This method tokenizes an abstract from a single url. Wait time is an integer number of seconds you\n",
    "    want to wait for the page to load.\n",
    "    \"\"\"\n",
    "    #driver = webdriver.Chrome()\n",
    "    #abstract_text = extractor(url,wait_time)\n",
    "    test_abstract = abstract.lower()\n",
    "    test_abstract = re.sub('[^a-zA-Z]', ' ', test_abstract) \n",
    "    test_abstract = re.sub(r'\\s+', ' ', test_abstract)\n",
    "        \n",
    "    abstract_tokens = nltk.word_tokenize(test_abstract)\n",
    "\n",
    "    text_tokens = [tkn for tkn in abstract_tokens if tkn not in stopwords.words('english')]\n",
    "    #driver.quit()\n",
    "    return text_tokens\n",
    "\n",
    "def cosine_scores(search_terms,text_tokens,model,n_tokens):\n",
    "    \"\"\"\n",
    "    Extracts the top n most similar tokens and their respective cosine similarity scores by \n",
    "    comparing the tokens from a single abstract to the trained vocabulary.\n",
    "    Parameters:\n",
    "    search_terms: desired search terms written as a list of strings; \n",
    "    text_tokens: A list of tokens for the text_tokens;\n",
    "    model: The trained word2vec model;\n",
    "    n_tokens: the number of top most similar tokens you'd like to return\n",
    "    \"\"\"\n",
    "\n",
    "    store = defaultdict(int)\n",
    "    for word in search_terms:\n",
    "        for tkn in text_tokens:\n",
    "            store[tkn] += model.wv.similarity(word,tkn)\n",
    "    \n",
    "    # Orders dictionary from highest to lowest cosine similarity score\n",
    "    cos_scores = sorted(store.items() , reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # Extracts top 20 most similar tokens\n",
    "    return cos_scores[:int(n_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"organic.laser.url.csv\")\n",
    "\n",
    "first_condition = df['PDF only'].isnull() #NaN actually means that nothing is there, not a string\n",
    "second_condition = df['Score AC'] >= 0.0 \n",
    "third_condition = df['Score TG'] >= 0.0\n",
    "fourth_condition = df['Score WT'] >= 0.0\n",
    "keep = (first_condition & (second_condition | third_condition | fourth_condition )) #Masking of URLs with PDFs\n",
    "\n",
    "new_df = df[keep]\n",
    "new_df = new_df.reindex(columns = ['Date', 'URL', 'Abstract', 'Tokens', 'Cosine Top 20', 'Cumulative Sum', 'Score AC', 'Score TG', 'Score WT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "for i in range(0, len(new_df)):\n",
    "    print('On url', i)\n",
    "    driver.refresh()\n",
    "    time.sleep(2)\n",
    "    pub_date, abstract = extractor(new_df['URL'].iloc[i],3) #Extracting both publication date and abstract\n",
    "    new_df.iloc[i,2] = str(abstract) #Date is position 0 and Abstract is position 2 in the array\n",
    "    new_df.iloc[i,0] = str(pub_date)\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "#Need to save new_df as csv for later and abstracts as txt for tokenizer\n",
    "new_df.to_csv('Organic_Laser_Abstracts.csv')\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Abstracts.csv', index_col=[0])\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df.iloc[i,3] = str(single_abstract_tkzr(new_df.iloc[i,2])) #Single_abstract_tkzr repurposed for abstracts already in new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Organic_Laser_Tokens.csv', index=False) #Saving new_df after adding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = ''\n",
    "for i in range(0, len(new_df)):\n",
    "    abstracts += str(new_df.iloc[i,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Organic_Laser_Tokenizer.txt', \"w\") as output: #Save .txt file for tokenizer\n",
    "    output.write(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = new_df\n",
    "for i in range(0,len(test_df)):\n",
    "    test_df.iloc[i,3] = new_df.iloc[i,3][1:-1] #Getting rid of the brackets around the tokens in Tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(new_df)):\n",
    "    new_df.iloc[i,3] = new_df.iloc[i,3][1:-1] #Getting rid of the brackets around the tokens in Tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Organic_Laser_Tokens2.csv', index=False) #Saving new_df after eliminating brackets in Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Organic_Laser_Tokens2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(word):\n",
    "    word = word[1:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Word2Vec model\n",
    "tokens = []\n",
    "for i in range(0,len(new_df)):\n",
    "    string_of_tokens = new_df.iloc[i,3]\n",
    "    string_of_tokens = string_of_tokens.split(', ')\n",
    "    new_tokens = list(map(remove_apostrophe, string_of_tokens)) #Removing the apostrophes in each token\n",
    "    tokens.append(new_tokens) #\"tokens\" is the list of tokens from each abstract to be used for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, size=100, min_count=1, iter=30) #training model with token list from above\n",
    "vocabulary = list(model.wv.vocab) #saving vocabulary as list for visualization\n",
    "print(model)\n",
    "model.save(\"NLP_Project_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cosine score\n",
    "tokens_cs = []\n",
    "for i in range(0,len(new_df)):\n",
    "    string_of_tokens_cs = new_df.iloc[i,3]\n",
    "    string_of_tokens_cs = string_of_tokens_cs.split(', ')\n",
    "    new_tokens_cs = list(map(remove_apostrophe, string_of_tokens_cs)) #Removing the apostrophes in each token for cosine score\n",
    "    tokens_cs.extend(new_tokens_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatized tokens for cosine score, was not source of error, turned out to be min_count\n",
    "lemmatized_tokens = []\n",
    "lemmatized_tokens = list(map(lemmatizer.lemmatize, tokens_cs))\n",
    "lemmatized_tokens.remove('successively') #'successively' would not be lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine scores of Top 20 tokens\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df['Cosine Top 20'].loc[i] = cosine_scores(['organic','laser'],tokens[i],model,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative Sum of Top 20\n",
    "for i in range(0, len(new_df)):\n",
    "    new_df['Cumulative Sum'].loc[i] = sum(i for j, i in new_df['Cosine Top 20'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer date strings to dates, and sort the list in ascending order of dates  \n",
    "data_df = new_df\n",
    "data_df['Date'] = pd.to_datetime(data_df['Date'])\n",
    "data_df.sort_values(by=['Date'], inplace=True, ascending=True)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('NLP_Organic_Laser_Data.csv', index=False) #Saving data_df after filling out all columns and organizing based on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('NLP_Organic_Laser_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Unsorted_Organic_laser_Data.csv', index=False) #Saving new_df with unsorted by date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Unsorted_Organic_laser_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Cumulative Top 20 Cosine Score for ['Organic', 'Laser'] versus Publication Date\n",
    "x = data_df['Date']\n",
    "y = data_df['Cumulative Sum']\n",
    "plt.xlabel('Date of Publication')\n",
    "plt.ylabel('Cumulative Top 20 Cosine Score')\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot with data not sorted by date\n",
    "x = new_df['Date']\n",
    "y = new_df['Cumulative Sum']\n",
    "plt.xlabel('Date of Publication')\n",
    "plt.ylabel('Cumulative Top 20 Cosine Score')\n",
    "plt.scatter(x[::],y[::], s=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TG Notes\n",
    "#Train model for a certain amount of epochs (saving them each time .save). \n",
    "#Observing the cosine score between two words in the W2V vocabulary.\n",
    "#model.similarity('france', 'spain')\n",
    "#words to compare: laser, plqy, photoluminescence quantum yield, amplified spontaneous emission, ase, lasing threshold\n",
    "#model = gensim.models.Word2Vec.load(\"NLP_Project.model\")\n",
    "#model = Word2Vec(tokens, size=100, min_count=1, iter=30)\n",
    "#model.save(\"NLP_Project.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Issue now is that I am not sure how to save the model at each epoch of training. \n",
    "#I think I am just overwriting it each time I go through the for loop.\n",
    "#model_test = [] list of models could work?\n",
    "#NLP_Project_test.model is just the model trained above that has been renamed\n",
    "test_array = np.empty((101,2))\n",
    "test_array_df = pd.DataFrame(data=test_array[0:,:], columns = {'Epochs', 'Cosine Score'})\n",
    "\n",
    "for i in range(len(test_array_df)):\n",
    "    test_array_df['Epochs'].iloc[i] = (1+i)*30\n",
    "    \n",
    "    model_test = gensim.models.Word2Vec.load(\"NLP_Project_test.model\") #model_test.append to add models to list\n",
    "    model_test = model.train.model_test\n",
    "    \n",
    "    test_array_df['Cosine Score'].iloc[i] = model_test.wv.similarity('emission', 'laser')\n",
    "    \n",
    "    print('On epoch:', i)\n",
    "    model.save(\"NLP_Project_test.model\")\n",
    "\n",
    "test_array_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
