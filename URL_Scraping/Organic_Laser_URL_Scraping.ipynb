{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f24c5e0e2323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www-sciencedirect-com.offcampus.lib.washington.edu/science/article/pii/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mfirst_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Copy/Paste the ScienceDirect URL here: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input filename with .txt extension you wish to store urls in: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code is used to scrape ScienceDirect of publication urls and write them to\n",
    "a text file in the current directory for later use.\n",
    "\n",
    "To use this code, go to ScienceDirect.com and search for the topic of interest.\n",
    "Then, copy the URL and paste it into terminal when prompted for user input.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_page(driver):\n",
    "    \"\"\"\n",
    "    This method finds all hrefs on webpage\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver (Selenium webdriver object) : Instance of the webdriver class e.g.\n",
    "        webdriver.Chrome()\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    elems (list) : A list of all scraped hrefs from the page\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "    return elems\n",
    "\n",
    "\n",
    "def clean(elems):\n",
    "    \"\"\"\n",
    "    This method takes a list of scraped selenium web elements\n",
    "    and filters/ returns only the hrefs leading to publications.\n",
    "\n",
    "    Filtering includes removing all urls with keywords that are indicative of\n",
    "    non-html links.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    elems (list) : The list of hrefs to be filtered\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    urls (list) : The new list of hrefs, which should be the same as the list\n",
    "        displayed on gui ScienceDirect\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    urls = []\n",
    "    for elem in elems:\n",
    "        url = elem.get_attribute(\"href\")\n",
    "        if 'article' in url and 'pdf' not in url\\\n",
    "                            and 'search' not in url\\\n",
    "                            and 'show=' not in url:\n",
    "            urls.append(url)\n",
    "    return urls\n",
    "\n",
    "def build_annual_urls(first_url,year):\n",
    "    \"\"\"\n",
    "    This method takes the first SD url and creates a list of\n",
    "    urls which lead to the following pages on SD which will be\n",
    "    scraped. The page list is for a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_url (str) : The URL from ScienceDirect after searching for desired\n",
    "        keywords\n",
    "\n",
    "    year (str or int) : The given year the URL list is being built for\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    page_urls (list) : List of urls the webdriver will use to access all\n",
    "        available pages for a given year on a given topic\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    page_urls = []\n",
    "    for i in range(7):\n",
    "        url_100 = first_url.replace('&show=25','&show=100')\n",
    "        urli = url_100 + '&offset=' + str(i) + '00' + '&articleTypes=REV%2CFLA' + '&years=' + str(year)\n",
    "        \n",
    "        page_urls.append(urli)\n",
    "\n",
    "    return page_urls\n",
    "\n",
    "def scrape_all(first_url,driver,year):\n",
    "    \"\"\"\n",
    "    This method takes the first ScienceDirect url and navigates\n",
    "    through all 60 pages of listed publications, scraping each url\n",
    "    on each page. Returns a list of the urls. Scrapes all urls for a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_url (str) : The very first ScienceDirect URL after keyword search\n",
    "\n",
    "    driver (Selenium webdriver object) : Instance of a selenium webdriver\n",
    "        e.g. webdrive.Chrome()\n",
    "\n",
    "    year (str or int) : The current year being scraped\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    urls (list) : A list of all collected urls for a given year\n",
    "\n",
    "    \"\"\"\n",
    "    page_list = build_annual_urls(first_url,year)\n",
    "    urls = []\n",
    "    for page in page_list:\n",
    "        driver.get(page)\n",
    "        time.sleep(1) #must sleep to allow page to load\n",
    "        elems = scrape_page(driver)\n",
    "        links = clean(elems)\n",
    "        if len(links) < 2:\n",
    "            break\n",
    "        for link in links:\n",
    "            urls.append(link)\n",
    "\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def proxify(scraped_urls,prefix):\n",
    "    \"\"\"\n",
    "    This method takes a list of scraped urls and turns them into urls that\n",
    "    go through the UW Library proxy so that all of them are full access.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scraped_urls (list) : The list of URLs to be converted\n",
    "\n",
    "    prefix (str) : The string that all URLs which go through the UW Library\n",
    "        Proxy start with.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    proxy_urls (list) : The list of converted URLs which go through UW Library\n",
    "        proxy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    proxy_urls = []\n",
    "    for url in scraped_urls:\n",
    "        sd_id = url[-17:]\n",
    "        newlink = prefix + sd_id\n",
    "        if sd_id.startswith('S'):\n",
    "            proxy_urls.append(newlink)\n",
    "\n",
    "    return proxy_urls\n",
    "\n",
    "def write_urls(urls,file,year):\n",
    "    \"\"\"\n",
    "    This method takes a list of urls and writes them to a desired text file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls (list) : The list of URLs to be saved.\n",
    "\n",
    "    file (file object) : The opened .txt file which will be written to.\n",
    "\n",
    "    year (str or int) : The year associated with the publication date.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Does not return anything\n",
    "\n",
    "    \"\"\"\n",
    "    for link in urls:\n",
    "        line = link\n",
    "        file.write(line)\n",
    "        file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "prefix = 'https://www-sciencedirect-com.offcampus.lib.washington.edu/science/article/pii/'\n",
    "\n",
    "first_url = input(\"Copy/Paste the ScienceDirect URL here: \")\n",
    "print('\\n')\n",
    "filename = input(\"Input filename with .txt extension you wish to store urls in: \")\n",
    "\n",
    "master_list = []\n",
    "years = np.arange(2011,2020)\n",
    "file = open(filename,'w')\n",
    "\n",
    "for year in years:\n",
    "    year = str(year)\n",
    "    scraped_urls = scrape_all(first_url,driver,year)\n",
    "    proxy_urls = proxify(scraped_urls,prefix)\n",
    "    for link in proxy_urls:\n",
    "        master_list.append(link)\n",
    "    print('Number of URLs collected = ',len(master_list))\n",
    "    print('Year is: ',year)\n",
    "    write_urls(proxy_urls,file,year)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table('organic.laser.url.list.txt')\n",
    "df.to_excel('organic.laser.url.xlsx','URL List')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
